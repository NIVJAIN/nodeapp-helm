* 
* ==> Audit <==
* |---------|--------------------------------|----------|-----------|---------|-------------------------------|-------------------------------|
| Command |              Args              | Profile  |   User    | Version |          Start Time           |           End Time            |
|---------|--------------------------------|----------|-----------|---------|-------------------------------|-------------------------------|
| start   | --driver=docker                | minikube | pinkyjain | v1.24.0 | Wed, 12 Jan 2022 12:48:46 +08 | Wed, 12 Jan 2022 12:53:22 +08 |
|         | --alsologtostderr              |          |           |         |                               |                               |
| addons  |                                | minikube | pinkyjain | v1.24.0 | Wed, 12 Jan 2022 19:30:20 +08 | Wed, 12 Jan 2022 19:30:20 +08 |
| addons  | list                           | minikube | pinkyjain | v1.24.0 | Wed, 12 Jan 2022 19:30:25 +08 | Wed, 12 Jan 2022 19:30:25 +08 |
| addons  | enable ingress                 | minikube | pinkyjain | v1.24.0 | Wed, 12 Jan 2022 19:30:40 +08 | Wed, 12 Jan 2022 19:30:51 +08 |
| addons  | list                           | minikube | pinkyjain | v1.24.0 | Wed, 12 Jan 2022 19:31:18 +08 | Wed, 12 Jan 2022 19:31:18 +08 |
| stop    |                                | minikube | pinkyjain | v1.24.0 | Wed, 12 Jan 2022 19:35:29 +08 | Wed, 12 Jan 2022 19:35:42 +08 |
| start   | --driver=docker                | minikube | pinkyjain | v1.24.0 | Fri, 14 Jan 2022 21:53:04 +08 | Fri, 14 Jan 2022 21:53:21 +08 |
|         | --alsologtostderr              |          |           |         |                               |                               |
| addons  | list                           | minikube | pinkyjain | v1.24.0 | Fri, 14 Jan 2022 21:53:37 +08 | Fri, 14 Jan 2022 21:53:37 +08 |
| addons  | enable metrics-server          | minikube | pinkyjain | v1.24.0 | Fri, 14 Jan 2022 21:54:37 +08 | Fri, 14 Jan 2022 21:54:37 +08 |
| service | nodeapp-service                | minikube | pinkyjain | v1.24.0 | Fri, 14 Jan 2022 21:55:37 +08 | Fri, 14 Jan 2022 21:55:48 +08 |
| stop    |                                | minikube | pinkyjain | v1.24.0 | Fri, 14 Jan 2022 21:59:41 +08 | Fri, 14 Jan 2022 21:59:54 +08 |
| start   |                                | minikube | pinkyjain | v1.24.0 | Fri, 21 Jan 2022 19:56:40 +08 | Fri, 21 Jan 2022 19:57:02 +08 |
| config  | help                           | minikube | pinkyjain | v1.24.0 | Fri, 21 Jan 2022 19:59:31 +08 | Fri, 21 Jan 2022 19:59:31 +08 |
| config  | view                           | minikube | pinkyjain | v1.24.0 | Fri, 21 Jan 2022 19:59:49 +08 | Fri, 21 Jan 2022 19:59:49 +08 |
| config  | view memory                    | minikube | pinkyjain | v1.24.0 | Fri, 21 Jan 2022 19:59:54 +08 | Fri, 21 Jan 2022 19:59:54 +08 |
| stop    |                                | minikube | pinkyjain | v1.24.0 | Fri, 21 Jan 2022 20:09:38 +08 | Fri, 21 Jan 2022 20:09:51 +08 |
| start   |                                | minikube | pinkyjain | v1.24.0 | Fri, 21 Jan 2022 20:10:38 +08 | Fri, 21 Jan 2022 20:11:00 +08 |
| stop    |                                | minikube | pinkyjain | v1.24.0 | Fri, 21 Jan 2022 21:20:35 +08 | Fri, 21 Jan 2022 21:20:48 +08 |
| stop    |                                | minikube | pinkyjain | v1.24.0 | Sat, 22 Jan 2022 08:45:37 +08 | Sat, 22 Jan 2022 08:45:50 +08 |
| delete  |                                | minikube | pinkyjain | v1.24.0 | Sat, 22 Jan 2022 09:14:34 +08 | Sat, 22 Jan 2022 09:14:34 +08 |
| start   | --driver=docker                | minikube | pinkyjain | v1.24.0 | Sat, 22 Jan 2022 12:53:42 +08 | Sat, 22 Jan 2022 12:54:03 +08 |
| addons  | list                           | minikube | pinkyjain | v1.24.0 | Sat, 22 Jan 2022 13:46:35 +08 | Sat, 22 Jan 2022 13:46:35 +08 |
| addons  | enable ingress                 | minikube | pinkyjain | v1.24.0 | Sat, 22 Jan 2022 13:46:43 +08 | Sat, 22 Jan 2022 13:46:44 +08 |
| addons  | list                           | minikube | pinkyjain | v1.24.0 | Sat, 22 Jan 2022 13:47:17 +08 | Sat, 22 Jan 2022 13:47:18 +08 |
| start   | --driver=docker                | minikube | pinkyjain | v1.24.0 | Sun, 23 Jan 2022 10:58:36 +08 | Sun, 23 Jan 2022 10:58:57 +08 |
| stop    |                                | minikube | pinkyjain | v1.24.0 | Sun, 23 Jan 2022 11:19:49 +08 | Sun, 23 Jan 2022 11:20:02 +08 |
| start   | --driver=docker                | minikube | pinkyjain | v1.24.0 | Sun, 23 Jan 2022 11:20:25 +08 | Sun, 23 Jan 2022 11:20:45 +08 |
| addons  | list                           | minikube | pinkyjain | v1.24.0 | Sun, 23 Jan 2022 17:03:09 +08 | Sun, 23 Jan 2022 17:03:09 +08 |
| stop    |                                | minikube | pinkyjain | v1.24.0 | Sun, 23 Jan 2022 17:11:19 +08 | Sun, 23 Jan 2022 17:11:32 +08 |
| delete  |                                | minikube | pinkyjain | v1.24.0 | Sun, 23 Jan 2022 17:11:52 +08 | Sun, 23 Jan 2022 17:11:56 +08 |
| start   |                                | minikube | pinkyjain | v1.24.0 | Sun, 23 Jan 2022 17:51:45 +08 | Sun, 23 Jan 2022 17:52:32 +08 |
| start   |                                | minikube | pinkyjain | v1.24.0 | Tue, 08 Feb 2022 20:29:26 +08 | Tue, 08 Feb 2022 20:29:43 +08 |
| stop    |                                | minikube | pinkyjain | v1.24.0 | Tue, 08 Feb 2022 21:06:12 +08 | Tue, 08 Feb 2022 21:06:25 +08 |
| start   |                                | minikube | pinkyjain | v1.24.0 | Sun, 03 Apr 2022 15:15:39 +08 | Sun, 03 Apr 2022 15:16:40 +08 |
| ip      |                                | minikube | pinkyjain | v1.24.0 | Sun, 03 Apr 2022 16:03:47 +08 | Sun, 03 Apr 2022 16:03:47 +08 |
| service | nodeapp-service                | minikube | pinkyjain | v1.24.0 | Sun, 03 Apr 2022 16:05:48 +08 | Sun, 03 Apr 2022 16:06:15 +08 |
| addons  | list                           | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 12:14:58 +08 | Mon, 04 Apr 2022 12:14:58 +08 |
| addons  | enable ingress                 | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 12:15:20 +08 | Mon, 04 Apr 2022 12:15:35 +08 |
| ip      |                                | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 12:18:21 +08 | Mon, 04 Apr 2022 12:18:22 +08 |
| ip      |                                | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 12:19:02 +08 | Mon, 04 Apr 2022 12:19:02 +08 |
| ip      |                                | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 12:19:42 +08 | Mon, 04 Apr 2022 12:19:42 +08 |
| ip      |                                | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 12:22:28 +08 | Mon, 04 Apr 2022 12:22:28 +08 |
| addons  | list                           | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 13:38:55 +08 | Mon, 04 Apr 2022 13:38:55 +08 |
| addons  | enable ingress-dns             | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 13:39:28 +08 | Mon, 04 Apr 2022 13:39:29 +08 |
| addons  | list                           | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 13:39:35 +08 | Mon, 04 Apr 2022 13:39:35 +08 |
| ip      |                                | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 13:45:01 +08 | Mon, 04 Apr 2022 13:45:01 +08 |
| ip      |                                | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 13:50:44 +08 | Mon, 04 Apr 2022 13:50:44 +08 |
| ip      |                                | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 13:50:59 +08 | Mon, 04 Apr 2022 13:50:59 +08 |
| tunnel  |                                | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 14:21:21 +08 | Mon, 04 Apr 2022 14:23:12 +08 |
| tunnel  |                                | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 14:23:13 +08 | Mon, 04 Apr 2022 14:30:16 +08 |
| tunnel  |                                | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 14:30:18 +08 | Mon, 04 Apr 2022 17:24:12 +08 |
| profile | list                           | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 17:25:04 +08 | Mon, 04 Apr 2022 17:25:04 +08 |
| addons  | enable ingress                 | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 17:26:21 +08 | Mon, 04 Apr 2022 17:26:22 +08 |
| service | nodeapp-service                | minikube | pinkyjain | v1.24.0 | Mon, 04 Apr 2022 17:31:02 +08 | Mon, 04 Apr 2022 17:31:07 +08 |
| start   |                                | minikube | pinkyjain | v1.24.0 | Tue, 05 Apr 2022 07:40:03 +08 | Tue, 05 Apr 2022 07:40:33 +08 |
| ssh     |                                | minikube | pinkyjain | v1.24.0 | Wed, 06 Apr 2022 17:24:49 +08 | Wed, 06 Apr 2022 17:25:26 +08 |
| ssh     |                                | minikube | pinkyjain | v1.24.0 | Sat, 09 Apr 2022 15:44:00 +08 | Sat, 09 Apr 2022 15:44:18 +08 |
| start   |                                | minikube | pinkyjain | v1.24.0 | Sat, 09 Apr 2022 16:41:21 +08 | Sat, 09 Apr 2022 16:42:15 +08 |
| start   |                                | minikube | pinkyjain | v1.24.0 | Sun, 10 Apr 2022 21:38:24 +08 | Sun, 10 Apr 2022 21:38:41 +08 |
|---------|--------------------------------|----------|-----------|---------|-------------------------------|-------------------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/04/10 21:38:24
Running on machine: Pinkys-MBP
Binary: Built with gc go1.17.2 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0410 21:38:24.643303    4927 out.go:297] Setting OutFile to fd 1 ...
I0410 21:38:24.643766    4927 out.go:349] isatty.IsTerminal(1) = true
I0410 21:38:24.643768    4927 out.go:310] Setting ErrFile to fd 2...
I0410 21:38:24.643771    4927 out.go:349] isatty.IsTerminal(2) = true
I0410 21:38:24.643855    4927 root.go:313] Updating PATH: /Users/pinkyjain/.minikube/bin
I0410 21:38:24.644664    4927 out.go:304] Setting JSON to false
I0410 21:38:24.675759    4927 start.go:112] hostinfo: {"hostname":"Pinkys-MBP","uptime":352,"bootTime":1649597552,"procs":440,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"12.2.1","kernelVersion":"21.3.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"5b662f11-6b96-5937-9b16-ab09a2df929a"}
W0410 21:38:24.675916    4927 start.go:120] gopshost.Virtualization returned error: not implemented yet
I0410 21:38:24.697506    4927 out.go:176] üòÑ  minikube v1.24.0 on Darwin 12.2.1 (arm64)
W0410 21:38:24.697998    4927 preload.go:294] Failed to list preload files: open /Users/pinkyjain/.minikube/cache/preloaded-tarball: no such file or directory
I0410 21:38:24.698007    4927 notify.go:174] Checking for updates...
I0410 21:38:24.698739    4927 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0410 21:38:24.699496    4927 driver.go:343] Setting default libvirt URI to qemu:///system
I0410 21:38:24.794770    4927 docker.go:132] docker version: linux-20.10.13
I0410 21:38:24.795141    4927 cli_runner.go:115] Run: docker system info --format "{{json .}}"
I0410 21:38:24.828769    4927 lock.go:35] WriteFile acquiring /Users/pinkyjain/.minikube/last_update_check: {Name:mk4d290ecad4892f2739ba83d28806744a7af675 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0410 21:38:24.852221    4927 out.go:176] üéâ  minikube 1.25.2 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.25.2
I0410 21:38:24.874193    4927 out.go:176] üí°  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0410 21:38:25.177452    4927 info.go:263] docker info: {ID:T5ZA:P4NB:QHBS:WOH3:73I4:JEPY:2H6J:2CD3:IXU6:W3US:XOF2:UZ4F Containers:17 ContainersRunning:7 ContainersPaused:0 ContainersStopped:10 Images:15 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:86 OomKillDisable:false NGoroutines:82 SystemTime:2022-04-10 13:38:24.899470381 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.104-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:5 MemTotal:25177460736 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.13 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2a1d4dbdb2a1030dc5b01e96fb110a9d9f150ecc Expected:2a1d4dbdb2a1030dc5b01e96fb110a9d9f150ecc} RuncCommit:{ID:v1.0.3-0-gf46b6ba Expected:v1.0.3-0-gf46b6ba} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.3.3] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0410 21:38:25.216956    4927 out.go:176] ‚ú®  Using the docker driver based on existing profile
I0410 21:38:25.216978    4927 start.go:280] selected driver: docker
I0410 21:38:25.216980    4927 start.go:762] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:16300 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:true ingress-dns:true istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[IngressController:ingress-nginx/controller:v1.0.4@sha256:545cff00370f28363dad31e3b59a94ba377854d3a11f18988f5f9e56841ef9ef IngressDNS:k8s-minikube/minikube-ingress-dns:0.0.2@sha256:4abe27f9fc03fedab1d655e2020e6b165faf3bf6de1088ce6cf215a75b78f05f KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host}
I0410 21:38:25.217027    4927 start.go:773] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc:}
I0410 21:38:25.217181    4927 cli_runner.go:115] Run: docker system info --format "{{json .}}"
I0410 21:38:25.376904    4927 info.go:263] docker info: {ID:T5ZA:P4NB:QHBS:WOH3:73I4:JEPY:2H6J:2CD3:IXU6:W3US:XOF2:UZ4F Containers:17 ContainersRunning:7 ContainersPaused:0 ContainersStopped:10 Images:15 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:86 OomKillDisable:false NGoroutines:82 SystemTime:2022-04-10 13:38:25.31127484 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.104-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:5 MemTotal:25177460736 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.13 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2a1d4dbdb2a1030dc5b01e96fb110a9d9f150ecc Expected:2a1d4dbdb2a1030dc5b01e96fb110a9d9f150ecc} RuncCommit:{ID:v1.0.3-0-gf46b6ba Expected:v1.0.3-0-gf46b6ba} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.8.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.3.3] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
W0410 21:38:25.377068    4927 info.go:50] Unable to get CPU info: no such file or directory
W0410 21:38:25.379733    4927 start.go:925] could not get system cpu info while verifying memory limits, which might be okay: no such file or directory
I0410 21:38:25.379782    4927 cni.go:93] Creating CNI manager for ""
I0410 21:38:25.379795    4927 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I0410 21:38:25.379813    4927 start_flags.go:282] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:16300 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:true ingress-dns:true istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[IngressController:ingress-nginx/controller:v1.0.4@sha256:545cff00370f28363dad31e3b59a94ba377854d3a11f18988f5f9e56841ef9ef IngressDNS:k8s-minikube/minikube-ingress-dns:0.0.2@sha256:4abe27f9fc03fedab1d655e2020e6b165faf3bf6de1088ce6cf215a75b78f05f KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host}
I0410 21:38:25.420996    4927 out.go:176] üëç  Starting control plane node minikube in cluster minikube
I0410 21:38:25.421079    4927 cache.go:118] Beginning downloading kic base image for docker with docker
I0410 21:38:25.441102    4927 out.go:176] üöú  Pulling base image ...
I0410 21:38:25.441294    4927 preload.go:132] Checking if preload exists for k8s version v1.22.3 and runtime docker
I0410 21:38:25.441476    4927 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon
I0410 21:38:25.550956    4927 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c in local docker daemon, skipping pull
I0410 21:38:25.551248    4927 cache.go:140] gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c exists in daemon, skipping load
W0410 21:38:25.746315    4927 preload.go:115] https://storage.googleapis.com/minikube-preloaded-volume-tarballs/preloaded-images-k8s-v13-v1.22.3-docker-overlay2-arm64.tar.lz4 status code: 404
I0410 21:38:25.747098    4927 profile.go:147] Saving config to /Users/pinkyjain/.minikube/profiles/minikube/config.json ...
I0410 21:38:25.747179    4927 cache.go:107] acquiring lock: {Name:mk7ac51055014feff671adcccdde39e15977296f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0410 21:38:25.747385    4927 cache.go:107] acquiring lock: {Name:mk9fd295b6729c63d846c958e6f269a2ce9c3027 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0410 21:38:25.748414    4927 cache.go:107] acquiring lock: {Name:mk825d021b98b16803db5645821f9957cbc2d631 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0410 21:38:25.748440    4927 cache.go:107] acquiring lock: {Name:mkf99396320cf14935160b2b179472b7d46d5f3c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0410 21:38:25.748716    4927 cache.go:107] acquiring lock: {Name:mk492bdab67b35f842c7c93049d975f42ef76a46 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0410 21:38:25.748904    4927 cache.go:107] acquiring lock: {Name:mk67b313d12259d64b29acca6447f5cf7d7ccb68 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0410 21:38:25.748992    4927 cache.go:107] acquiring lock: {Name:mk86519257b9f6a858395c79dcc6ecb68bd7591e Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0410 21:38:25.749473    4927 cache.go:107] acquiring lock: {Name:mk51af28179683db45891eb612ef3be297ef705a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0410 21:38:25.749494    4927 cache.go:107] acquiring lock: {Name:mkd822ed29551504c47967e88871b99940c10ede Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0410 21:38:25.749675    4927 cache.go:115] /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/kube-proxy_v1.22.3 exists
I0410 21:38:25.749677    4927 cache.go:115] /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/etcd_3.5.0-0 exists
I0410 21:38:25.749675    4927 cache.go:115] /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/kube-scheduler_v1.22.3 exists
I0410 21:38:25.749704    4927 cache.go:206] Successfully downloaded all kic artifacts
I0410 21:38:25.749718    4927 cache.go:96] cache image "k8s.gcr.io/kube-scheduler:v1.22.3" -> "/Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/kube-scheduler_v1.22.3" took 1.309708ms
I0410 21:38:25.749713    4927 cache.go:96] cache image "k8s.gcr.io/kube-proxy:v1.22.3" -> "/Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/kube-proxy_v1.22.3" took 864.208¬µs
I0410 21:38:25.749716    4927 cache.go:96] cache image "k8s.gcr.io/etcd:3.5.0-0" -> "/Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/etcd_3.5.0-0" took 728.167¬µs
I0410 21:38:25.749724    4927 cache.go:80] save to tar file k8s.gcr.io/kube-proxy:v1.22.3 -> /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/kube-proxy_v1.22.3 succeeded
I0410 21:38:25.749727    4927 cache.go:80] save to tar file k8s.gcr.io/etcd:3.5.0-0 -> /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/etcd_3.5.0-0 succeeded
I0410 21:38:25.749743    4927 cache.go:115] /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/pause_3.5 exists
I0410 21:38:25.749751    4927 cache.go:96] cache image "k8s.gcr.io/pause:3.5" -> "/Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/pause_3.5" took 2.372416ms
I0410 21:38:25.749753    4927 cache.go:80] save to tar file k8s.gcr.io/pause:3.5 -> /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/pause_3.5 succeeded
I0410 21:38:25.749760    4927 cache.go:115] /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/kube-apiserver_v1.22.3 exists
I0410 21:38:25.749753    4927 start.go:313] acquiring machines lock for minikube: {Name:mk98948ae0847d3b3b160ddee96fde1a2530168f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0410 21:38:25.749765    4927 cache.go:115] /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/kube-controller-manager_v1.22.3 exists
I0410 21:38:25.749773    4927 cache.go:80] save to tar file k8s.gcr.io/kube-scheduler:v1.22.3 -> /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/kube-scheduler_v1.22.3 succeeded
I0410 21:38:25.749777    4927 cache.go:96] cache image "k8s.gcr.io/kube-controller-manager:v1.22.3" -> "/Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/kube-controller-manager_v1.22.3" took 1.382042ms
I0410 21:38:25.749781    4927 cache.go:80] save to tar file k8s.gcr.io/kube-controller-manager:v1.22.3 -> /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/kube-controller-manager_v1.22.3 succeeded
I0410 21:38:25.749776    4927 cache.go:96] cache image "k8s.gcr.io/kube-apiserver:v1.22.3" -> "/Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/kube-apiserver_v1.22.3" took 1.104958ms
I0410 21:38:25.749790    4927 cache.go:80] save to tar file k8s.gcr.io/kube-apiserver:v1.22.3 -> /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/kube-apiserver_v1.22.3 succeeded
I0410 21:38:25.749832    4927 cache.go:107] acquiring lock: {Name:mkba1633f5f6791a7df92f49d27194499edffab7 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0410 21:38:25.749948    4927 cache.go:115] /Users/pinkyjain/.minikube/cache/images/docker.io/kubernetesui/metrics-scraper_v1.0.7 exists
I0410 21:38:25.749951    4927 cache.go:115] /Users/pinkyjain/.minikube/cache/images/docker.io/kubernetesui/dashboard_v2.3.1 exists
I0410 21:38:25.749957    4927 cache.go:96] cache image "docker.io/kubernetesui/metrics-scraper:v1.0.7" -> "/Users/pinkyjain/.minikube/cache/images/docker.io/kubernetesui/metrics-scraper_v1.0.7" took 2.833708ms
I0410 21:38:25.749966    4927 cache.go:80] save to tar file docker.io/kubernetesui/metrics-scraper:v1.0.7 -> /Users/pinkyjain/.minikube/cache/images/docker.io/kubernetesui/metrics-scraper_v1.0.7 succeeded
I0410 21:38:25.749970    4927 start.go:317] acquired machines lock for "minikube" in 204.709¬µs
I0410 21:38:25.749962    4927 cache.go:96] cache image "docker.io/kubernetesui/dashboard:v2.3.1" -> "/Users/pinkyjain/.minikube/cache/images/docker.io/kubernetesui/dashboard_v2.3.1" took 479.584¬µs
I0410 21:38:25.749975    4927 cache.go:80] save to tar file docker.io/kubernetesui/dashboard:v2.3.1 -> /Users/pinkyjain/.minikube/cache/images/docker.io/kubernetesui/dashboard_v2.3.1 succeeded
I0410 21:38:25.749986    4927 start.go:93] Skipping create...Using existing machine configuration
I0410 21:38:25.750009    4927 fix.go:55] fixHost starting: 
I0410 21:38:25.750049    4927 cache.go:115] /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/coredns/coredns_v1.8.4 exists
I0410 21:38:25.750054    4927 cache.go:96] cache image "k8s.gcr.io/coredns/coredns:v1.8.4" -> "/Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/coredns/coredns_v1.8.4" took 279.875¬µs
I0410 21:38:25.750059    4927 cache.go:80] save to tar file k8s.gcr.io/coredns/coredns:v1.8.4 -> /Users/pinkyjain/.minikube/cache/images/k8s.gcr.io/coredns/coredns_v1.8.4 succeeded
I0410 21:38:25.750077    4927 cache.go:115] /Users/pinkyjain/.minikube/cache/images/gcr.io/k8s-minikube/storage-provisioner_v5 exists
I0410 21:38:25.750086    4927 cache.go:96] cache image "gcr.io/k8s-minikube/storage-provisioner:v5" -> "/Users/pinkyjain/.minikube/cache/images/gcr.io/k8s-minikube/storage-provisioner_v5" took 651.375¬µs
I0410 21:38:25.750094    4927 cache.go:80] save to tar file gcr.io/k8s-minikube/storage-provisioner:v5 -> /Users/pinkyjain/.minikube/cache/images/gcr.io/k8s-minikube/storage-provisioner_v5 succeeded
I0410 21:38:25.750097    4927 cache.go:87] Successfully saved all images to host disk.
I0410 21:38:25.750282    4927 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0410 21:38:25.860385    4927 fix.go:108] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0410 21:38:25.860455    4927 fix.go:134] unexpected machine state, will restart: <nil>
I0410 21:38:25.898376    4927 out.go:176] üîÑ  Restarting existing docker container for "minikube" ...
I0410 21:38:25.898466    4927 cli_runner.go:115] Run: docker start minikube
I0410 21:38:26.291462    4927 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0410 21:38:26.402858    4927 kic.go:420] container "minikube" state is running.
I0410 21:38:26.403479    4927 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0410 21:38:26.512677    4927 profile.go:147] Saving config to /Users/pinkyjain/.minikube/profiles/minikube/config.json ...
I0410 21:38:26.513001    4927 machine.go:88] provisioning docker machine ...
I0410 21:38:26.513007    4927 ubuntu.go:169] provisioning hostname "minikube"
I0410 21:38:26.513072    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:26.624316    4927 main.go:130] libmachine: Using SSH client type: native
I0410 21:38:26.624641    4927 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x10253ac20] 0x10253da40 <nil>  [] 0s} 127.0.0.1 49546 <nil> <nil>}
I0410 21:38:26.624647    4927 main.go:130] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0410 21:38:26.755575    4927 main.go:130] libmachine: SSH cmd err, output: <nil>: minikube

I0410 21:38:26.755688    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:26.865245    4927 main.go:130] libmachine: Using SSH client type: native
I0410 21:38:26.865436    4927 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x10253ac20] 0x10253da40 <nil>  [] 0s} 127.0.0.1 49546 <nil> <nil>}
I0410 21:38:26.865447    4927 main.go:130] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0410 21:38:26.978034    4927 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I0410 21:38:26.978057    4927 ubuntu.go:175] set auth options {CertDir:/Users/pinkyjain/.minikube CaCertPath:/Users/pinkyjain/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/pinkyjain/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/pinkyjain/.minikube/machines/server.pem ServerKeyPath:/Users/pinkyjain/.minikube/machines/server-key.pem ClientKeyPath:/Users/pinkyjain/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/pinkyjain/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/pinkyjain/.minikube}
I0410 21:38:26.978072    4927 ubuntu.go:177] setting up certificates
I0410 21:38:26.978083    4927 provision.go:83] configureAuth start
I0410 21:38:26.978157    4927 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0410 21:38:27.091870    4927 provision.go:138] copyHostCerts
I0410 21:38:27.092038    4927 exec_runner.go:144] found /Users/pinkyjain/.minikube/ca.pem, removing ...
I0410 21:38:27.092053    4927 exec_runner.go:207] rm: /Users/pinkyjain/.minikube/ca.pem
I0410 21:38:27.092159    4927 exec_runner.go:151] cp: /Users/pinkyjain/.minikube/certs/ca.pem --> /Users/pinkyjain/.minikube/ca.pem (1086 bytes)
I0410 21:38:27.092466    4927 exec_runner.go:144] found /Users/pinkyjain/.minikube/cert.pem, removing ...
I0410 21:38:27.092469    4927 exec_runner.go:207] rm: /Users/pinkyjain/.minikube/cert.pem
I0410 21:38:27.092520    4927 exec_runner.go:151] cp: /Users/pinkyjain/.minikube/certs/cert.pem --> /Users/pinkyjain/.minikube/cert.pem (1127 bytes)
I0410 21:38:27.092711    4927 exec_runner.go:144] found /Users/pinkyjain/.minikube/key.pem, removing ...
I0410 21:38:27.092713    4927 exec_runner.go:207] rm: /Users/pinkyjain/.minikube/key.pem
I0410 21:38:27.092755    4927 exec_runner.go:151] cp: /Users/pinkyjain/.minikube/certs/key.pem --> /Users/pinkyjain/.minikube/key.pem (1675 bytes)
I0410 21:38:27.092887    4927 provision.go:112] generating server cert: /Users/pinkyjain/.minikube/machines/server.pem ca-key=/Users/pinkyjain/.minikube/certs/ca.pem private-key=/Users/pinkyjain/.minikube/certs/ca-key.pem org=pinkyjain.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0410 21:38:27.291324    4927 provision.go:172] copyRemoteCerts
I0410 21:38:27.291637    4927 ssh_runner.go:152] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0410 21:38:27.291690    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:27.402029    4927 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49546 SSHKeyPath:/Users/pinkyjain/.minikube/machines/minikube/id_rsa Username:docker}
I0410 21:38:27.484217    4927 ssh_runner.go:319] scp /Users/pinkyjain/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0410 21:38:27.503274    4927 ssh_runner.go:319] scp /Users/pinkyjain/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I0410 21:38:27.517227    4927 ssh_runner.go:319] scp /Users/pinkyjain/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0410 21:38:27.531703    4927 provision.go:86] duration metric: configureAuth took 553.340125ms
I0410 21:38:27.531713    4927 ubuntu.go:193] setting minikube options for container-runtime
I0410 21:38:27.531888    4927 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0410 21:38:27.531941    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:27.641245    4927 main.go:130] libmachine: Using SSH client type: native
I0410 21:38:27.641394    4927 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x10253ac20] 0x10253da40 <nil>  [] 0s} 127.0.0.1 49546 <nil> <nil>}
I0410 21:38:27.641398    4927 main.go:130] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0410 21:38:27.753384    4927 main.go:130] libmachine: SSH cmd err, output: <nil>: overlay

I0410 21:38:27.753392    4927 ubuntu.go:71] root file system type: overlay
I0410 21:38:27.753587    4927 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0410 21:38:27.753682    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:27.863629    4927 main.go:130] libmachine: Using SSH client type: native
I0410 21:38:27.863786    4927 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x10253ac20] 0x10253da40 <nil>  [] 0s} 127.0.0.1 49546 <nil> <nil>}
I0410 21:38:27.863831    4927 main.go:130] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0410 21:38:27.982594    4927 main.go:130] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0410 21:38:27.982682    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:28.093181    4927 main.go:130] libmachine: Using SSH client type: native
I0410 21:38:28.093347    4927 main.go:130] libmachine: &{{{<nil> 0 [] [] []} docker [0x10253ac20] 0x10253da40 <nil>  [] 0s} 127.0.0.1 49546 <nil> <nil>}
I0410 21:38:28.093361    4927 main.go:130] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0410 21:38:28.205430    4927 main.go:130] libmachine: SSH cmd err, output: <nil>: 
I0410 21:38:28.205442    4927 machine.go:91] provisioned docker machine in 1.69243825s
I0410 21:38:28.205452    4927 start.go:267] post-start starting for "minikube" (driver="docker")
I0410 21:38:28.205455    4927 start.go:277] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0410 21:38:28.205534    4927 ssh_runner.go:152] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0410 21:38:28.205579    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:28.312664    4927 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49546 SSHKeyPath:/Users/pinkyjain/.minikube/machines/minikube/id_rsa Username:docker}
I0410 21:38:28.397210    4927 ssh_runner.go:152] Run: cat /etc/os-release
I0410 21:38:28.400547    4927 main.go:130] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0410 21:38:28.400554    4927 main.go:130] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0410 21:38:28.400559    4927 main.go:130] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0410 21:38:28.400561    4927 info.go:137] Remote host: Ubuntu 20.04.2 LTS
I0410 21:38:28.400566    4927 filesync.go:126] Scanning /Users/pinkyjain/.minikube/addons for local assets ...
I0410 21:38:28.400718    4927 filesync.go:126] Scanning /Users/pinkyjain/.minikube/files for local assets ...
I0410 21:38:28.400749    4927 start.go:270] post-start completed in 195.2935ms
I0410 21:38:28.400828    4927 ssh_runner.go:152] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0410 21:38:28.400854    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:28.506645    4927 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49546 SSHKeyPath:/Users/pinkyjain/.minikube/machines/minikube/id_rsa Username:docker}
I0410 21:38:28.586126    4927 fix.go:57] fixHost completed within 2.836115042s
I0410 21:38:28.586149    4927 start.go:80] releasing machines lock for "minikube", held for 2.836161792s
I0410 21:38:28.586238    4927 cli_runner.go:115] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0410 21:38:28.696944    4927 ssh_runner.go:152] Run: systemctl --version
I0410 21:38:28.697004    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:28.697078    4927 ssh_runner.go:152] Run: curl -sS -m 2 https://k8s.gcr.io/
I0410 21:38:28.697350    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:28.812577    4927 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49546 SSHKeyPath:/Users/pinkyjain/.minikube/machines/minikube/id_rsa Username:docker}
I0410 21:38:28.814150    4927 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49546 SSHKeyPath:/Users/pinkyjain/.minikube/machines/minikube/id_rsa Username:docker}
I0410 21:38:28.980440    4927 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service containerd
I0410 21:38:28.989073    4927 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I0410 21:38:28.997075    4927 cruntime.go:255] skipping containerd shutdown because we are bound to it
I0410 21:38:28.997172    4927 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service crio
I0410 21:38:29.007088    4927 ssh_runner.go:152] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I0410 21:38:29.040409    4927 ssh_runner.go:152] Run: sudo systemctl unmask docker.service
I0410 21:38:29.125559    4927 ssh_runner.go:152] Run: sudo systemctl enable docker.socket
I0410 21:38:29.185704    4927 ssh_runner.go:152] Run: sudo systemctl cat docker.service
I0410 21:38:29.195003    4927 ssh_runner.go:152] Run: sudo systemctl daemon-reload
I0410 21:38:29.245333    4927 ssh_runner.go:152] Run: sudo systemctl start docker
I0410 21:38:29.254811    4927 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I0410 21:38:29.359225    4927 ssh_runner.go:152] Run: docker version --format {{.Server.Version}}
I0410 21:38:29.436869    4927 out.go:203] üê≥  Preparing Kubernetes v1.22.3 on Docker 20.10.8 ...
I0410 21:38:29.437119    4927 cli_runner.go:115] Run: docker exec -t minikube dig +short host.docker.internal
I0410 21:38:29.639281    4927 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I0410 21:38:29.639415    4927 ssh_runner.go:152] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I0410 21:38:29.643301    4927 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0410 21:38:29.652334    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0410 21:38:29.760541    4927 preload.go:132] Checking if preload exists for k8s version v1.22.3 and runtime docker
I0410 21:38:29.760620    4927 ssh_runner.go:152] Run: docker images --format {{.Repository}}:{{.Tag}}
I0410 21:38:29.796591    4927 docker.go:558] Got preloaded images: -- stdout --
nivjain/nodeapp-helm:v1
mongo:latest
bitnami/mongodb:4.4.13-debian-10-r33
mongo:<none>
hyperledger/besu:latest
hyperledger/besu:<none>
mongo:<none>
k8s.gcr.io/kube-apiserver:v1.22.3
k8s.gcr.io/kube-proxy:v1.22.3
k8s.gcr.io/kube-controller-manager:v1.22.3
k8s.gcr.io/kube-scheduler:v1.22.3
gcr.io/k8s-minikube/minikube-ingress-dns:<none>
k8s.gcr.io/ingress-nginx/controller:<none>
k8s.gcr.io/ingress-nginx/kube-webhook-certgen:<none>
k8s.gcr.io/etcd:3.5.0-0
kubernetesui/dashboard:v2.3.1
kubernetesui/metrics-scraper:v1.0.7
k8s.gcr.io/coredns/coredns:v1.8.4
nivjain/k8s-nodeapp:v1
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/pause:3.5
pegasyseng/k8s-helper:v1.18.4

-- /stdout --
I0410 21:38:29.796601    4927 cache_images.go:79] Images are preloaded, skipping loading
I0410 21:38:29.796667    4927 ssh_runner.go:152] Run: docker info --format {{.CgroupDriver}}
I0410 21:38:29.994584    4927 cni.go:93] Creating CNI manager for ""
I0410 21:38:29.994598    4927 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I0410 21:38:29.994673    4927 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0410 21:38:29.994693    4927 kubeadm.go:153] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.22.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I0410 21:38:29.995379    4927 kubeadm.go:157] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.22.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0410 21:38:29.995783    4927 kubeadm.go:909] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.22.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0410 21:38:29.995940    4927 ssh_runner.go:152] Run: sudo ls /var/lib/minikube/binaries/v1.22.3
I0410 21:38:30.003344    4927 binaries.go:44] Found k8s binaries, skipping transfer
I0410 21:38:30.003444    4927 ssh_runner.go:152] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0410 21:38:30.009206    4927 ssh_runner.go:319] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (334 bytes)
I0410 21:38:30.019204    4927 ssh_runner.go:319] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0410 21:38:30.029259    4927 ssh_runner.go:319] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2050 bytes)
I0410 21:38:30.039511    4927 ssh_runner.go:152] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0410 21:38:30.043012    4927 ssh_runner.go:152] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0410 21:38:30.051720    4927 certs.go:54] Setting up /Users/pinkyjain/.minikube/profiles/minikube for IP: 192.168.49.2
I0410 21:38:30.052317    4927 certs.go:182] skipping minikubeCA CA generation: /Users/pinkyjain/.minikube/ca.key
I0410 21:38:30.052564    4927 certs.go:182] skipping proxyClientCA CA generation: /Users/pinkyjain/.minikube/proxy-client-ca.key
I0410 21:38:30.052644    4927 certs.go:298] skipping minikube-user signed cert generation: /Users/pinkyjain/.minikube/profiles/minikube/client.key
I0410 21:38:30.052876    4927 certs.go:298] skipping minikube signed cert generation: /Users/pinkyjain/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0410 21:38:30.053073    4927 certs.go:298] skipping aggregator signed cert generation: /Users/pinkyjain/.minikube/profiles/minikube/proxy-client.key
I0410 21:38:30.053296    4927 certs.go:388] found cert: /Users/pinkyjain/.minikube/certs/Users/pinkyjain/.minikube/certs/ca-key.pem (1679 bytes)
I0410 21:38:30.053322    4927 certs.go:388] found cert: /Users/pinkyjain/.minikube/certs/Users/pinkyjain/.minikube/certs/ca.pem (1086 bytes)
I0410 21:38:30.053340    4927 certs.go:388] found cert: /Users/pinkyjain/.minikube/certs/Users/pinkyjain/.minikube/certs/cert.pem (1127 bytes)
I0410 21:38:30.053362    4927 certs.go:388] found cert: /Users/pinkyjain/.minikube/certs/Users/pinkyjain/.minikube/certs/key.pem (1675 bytes)
I0410 21:38:30.054324    4927 ssh_runner.go:319] scp /Users/pinkyjain/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0410 21:38:30.068527    4927 ssh_runner.go:319] scp /Users/pinkyjain/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0410 21:38:30.083110    4927 ssh_runner.go:319] scp /Users/pinkyjain/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0410 21:38:30.097724    4927 ssh_runner.go:319] scp /Users/pinkyjain/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0410 21:38:30.114178    4927 ssh_runner.go:319] scp /Users/pinkyjain/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0410 21:38:30.128896    4927 ssh_runner.go:319] scp /Users/pinkyjain/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0410 21:38:30.142546    4927 ssh_runner.go:319] scp /Users/pinkyjain/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0410 21:38:30.156421    4927 ssh_runner.go:319] scp /Users/pinkyjain/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0410 21:38:30.170576    4927 ssh_runner.go:319] scp /Users/pinkyjain/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0410 21:38:30.184552    4927 ssh_runner.go:319] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0410 21:38:30.195893    4927 ssh_runner.go:152] Run: openssl version
I0410 21:38:30.203784    4927 ssh_runner.go:152] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0410 21:38:30.211506    4927 ssh_runner.go:152] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0410 21:38:30.214951    4927 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Jan 12 04:51 /usr/share/ca-certificates/minikubeCA.pem
I0410 21:38:30.215063    4927 ssh_runner.go:152] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0410 21:38:30.219421    4927 ssh_runner.go:152] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0410 21:38:30.225589    4927 kubeadm.go:390] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.28@sha256:4780f1897569d2bf77aafb3d133a08d42b4fe61127f06fcfc90c2c5d902d893c Memory:16300 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.22.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:true ingress-dns:true istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[IngressController:ingress-nginx/controller:v1.0.4@sha256:545cff00370f28363dad31e3b59a94ba377854d3a11f18988f5f9e56841ef9ef IngressDNS:k8s-minikube/minikube-ingress-dns:0.0.2@sha256:4abe27f9fc03fedab1d655e2020e6b165faf3bf6de1088ce6cf215a75b78f05f KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host}
I0410 21:38:30.225803    4927 ssh_runner.go:152] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0410 21:38:30.251625    4927 ssh_runner.go:152] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0410 21:38:30.258727    4927 kubeadm.go:401] found existing configuration files, will attempt cluster restart
I0410 21:38:30.258785    4927 kubeadm.go:600] restartCluster start
I0410 21:38:30.258872    4927 ssh_runner.go:152] Run: sudo test -d /data/minikube
I0410 21:38:30.265242    4927 kubeadm.go:126] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0410 21:38:30.265322    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0410 21:38:30.378592    4927 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:49720"
I0410 21:38:30.378605    4927 kubeconfig.go:116] verify returned: got: 127.0.0.1:49720, want: 127.0.0.1:49550
I0410 21:38:30.379313    4927 lock.go:35] WriteFile acquiring /Users/pinkyjain/.kube/config: {Name:mkd0a86ddd80edb0de0c54e4931456eaa58dda2e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0410 21:38:30.385400    4927 ssh_runner.go:152] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0410 21:38:30.392242    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:30.392299    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:30.402695    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:30.603818    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:30.604153    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:30.615325    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:30.803882    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:30.804181    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:30.815546    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:31.003851    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:31.004107    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:31.015228    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:31.203810    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:31.203992    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:31.213547    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:31.403874    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:31.404341    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:31.415201    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:31.603826    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:31.604114    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:31.614850    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:31.803228    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:31.803633    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:31.815753    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:32.003853    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:32.004105    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:32.015369    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:32.203844    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:32.204030    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:32.213155    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:32.403844    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:32.404248    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:32.415726    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:32.603609    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:32.603730    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:32.613709    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:32.803881    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:32.804156    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:32.816071    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:33.003831    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:33.004062    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:33.013327    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:33.203801    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:33.203957    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:33.213588    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:33.403833    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:33.404041    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:33.416151    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:33.416163    4927 api_server.go:165] Checking apiserver status ...
I0410 21:38:33.416279    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0410 21:38:33.426399    4927 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0410 21:38:33.426416    4927 kubeadm.go:575] needs reconfigure: apiserver error: timed out waiting for the condition
I0410 21:38:33.426426    4927 kubeadm.go:1032] stopping kube-system containers ...
I0410 21:38:33.426553    4927 ssh_runner.go:152] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0410 21:38:33.454231    4927 docker.go:390] Stopping containers: [99c7de4781e9 7ade39c2fbc6 7a0111e619b0 354eecbfc005 d66e2b3f1b71 aadf0cc9aad4 9ac4ec4ee1fb 8f93b028370a 8e47e9dcc299 b33da1b1e820 596b935cd69f babd5250bccc 4ef528135a5e 1af73714b0b6 f862c15a5f41 dde9f307f482 f97a239bcce5 d4c9594a1a96 5ad9b9316f8c 2ef9ab35e0f4 187b94277d3c 37aff5ebc77b f2e4dc31bbcc 7235d749a525 b407a109b98d 2aa24a441b35 dfb6d43fe12c c011175f5ec4 9bf38d34f76a 8a208785f3c7 81cbfe6d97f0]
I0410 21:38:33.454350    4927 ssh_runner.go:152] Run: docker stop 99c7de4781e9 7ade39c2fbc6 7a0111e619b0 354eecbfc005 d66e2b3f1b71 aadf0cc9aad4 9ac4ec4ee1fb 8f93b028370a 8e47e9dcc299 b33da1b1e820 596b935cd69f babd5250bccc 4ef528135a5e 1af73714b0b6 f862c15a5f41 dde9f307f482 f97a239bcce5 d4c9594a1a96 5ad9b9316f8c 2ef9ab35e0f4 187b94277d3c 37aff5ebc77b f2e4dc31bbcc 7235d749a525 b407a109b98d 2aa24a441b35 dfb6d43fe12c c011175f5ec4 9bf38d34f76a 8a208785f3c7 81cbfe6d97f0
I0410 21:38:33.481562    4927 ssh_runner.go:152] Run: sudo systemctl stop kubelet
I0410 21:38:33.490796    4927 ssh_runner.go:152] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0410 21:38:33.497532    4927 kubeadm.go:154] found existing configuration files:
-rw------- 1 root root 5639 Apr  3 07:16 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Apr  9 08:42 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5659 Apr  3 07:16 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Apr  9 08:42 /etc/kubernetes/scheduler.conf

I0410 21:38:33.497663    4927 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0410 21:38:33.506288    4927 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0410 21:38:33.513960    4927 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0410 21:38:33.520778    4927 kubeadm.go:165] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0410 21:38:33.520909    4927 ssh_runner.go:152] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0410 21:38:33.526327    4927 ssh_runner.go:152] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0410 21:38:33.532929    4927 kubeadm.go:165] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0410 21:38:33.533002    4927 ssh_runner.go:152] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0410 21:38:33.538461    4927 ssh_runner.go:152] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0410 21:38:33.544736    4927 kubeadm.go:676] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0410 21:38:33.544754    4927 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0410 21:38:33.657174    4927 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0410 21:38:33.914368    4927 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0410 21:38:34.047472    4927 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0410 21:38:34.092654    4927 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0410 21:38:34.132237    4927 api_server.go:51] waiting for apiserver process to appear ...
I0410 21:38:34.132370    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0410 21:38:34.642390    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0410 21:38:35.142310    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0410 21:38:35.642784    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0410 21:38:35.657205    4927 api_server.go:71] duration metric: took 1.524967125s to wait for apiserver process to appear ...
I0410 21:38:35.657221    4927 api_server.go:87] waiting for apiserver healthz status ...
I0410 21:38:35.657232    4927 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49550/healthz ...
I0410 21:38:35.658598    4927 api_server.go:256] stopped: https://127.0.0.1:49550/healthz: Get "https://127.0.0.1:49550/healthz": EOF
I0410 21:38:36.159327    4927 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49550/healthz ...
I0410 21:38:38.951232    4927 api_server.go:266] https://127.0.0.1:49550/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0410 21:38:38.951250    4927 api_server.go:102] status: https://127.0.0.1:49550/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0410 21:38:39.159316    4927 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49550/healthz ...
I0410 21:38:39.166574    4927 api_server.go:266] https://127.0.0.1:49550/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0410 21:38:39.166592    4927 api_server.go:102] status: https://127.0.0.1:49550/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0410 21:38:39.659467    4927 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49550/healthz ...
I0410 21:38:39.665753    4927 api_server.go:266] https://127.0.0.1:49550/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0410 21:38:39.665774    4927 api_server.go:102] status: https://127.0.0.1:49550/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0410 21:38:40.159400    4927 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49550/healthz ...
I0410 21:38:40.166122    4927 api_server.go:266] https://127.0.0.1:49550/healthz returned 200:
ok
I0410 21:38:40.179695    4927 api_server.go:140] control plane version: v1.22.3
I0410 21:38:40.179710    4927 api_server.go:130] duration metric: took 4.522485292s to wait for apiserver health ...
I0410 21:38:40.179724    4927 cni.go:93] Creating CNI manager for ""
I0410 21:38:40.179732    4927 cni.go:167] CNI unnecessary in this configuration, recommending no CNI
I0410 21:38:40.179739    4927 system_pods.go:43] waiting for kube-system pods to appear ...
I0410 21:38:40.198300    4927 system_pods.go:59] 8 kube-system pods found
I0410 21:38:40.198323    4927 system_pods.go:61] "coredns-78fcd69978-m2pqh" [141b0481-6d98-41bc-a7ac-a3751fe55978] Running
I0410 21:38:40.198332    4927 system_pods.go:61] "etcd-minikube" [79d53611-8f0a-4b93-a5f6-02bc73cb1d9e] Running
I0410 21:38:40.198340    4927 system_pods.go:61] "kube-apiserver-minikube" [b64312e9-1e75-4419-971a-b4a5d6153235] Running
I0410 21:38:40.198348    4927 system_pods.go:61] "kube-controller-manager-minikube" [6d0ed9a4-d50b-4aa7-af02-9418418138e1] Running
I0410 21:38:40.198356    4927 system_pods.go:61] "kube-ingress-dns-minikube" [e75af6ad-ff32-4304-9f8c-8528d04c7a08] Running
I0410 21:38:40.198364    4927 system_pods.go:61] "kube-proxy-kzwvq" [25752949-94fa-4a25-b668-e2ea8f56cbcf] Running
I0410 21:38:40.198371    4927 system_pods.go:61] "kube-scheduler-minikube" [a5f70102-3807-4672-aa82-c91fbf8d4e15] Running
I0410 21:38:40.198379    4927 system_pods.go:61] "storage-provisioner" [102b0da6-9771-45cc-936b-7b42d2b17803] Running
I0410 21:38:40.198385    4927 system_pods.go:74] duration metric: took 18.641333ms to wait for pod list to return data ...
I0410 21:38:40.198420    4927 node_conditions.go:102] verifying NodePressure condition ...
I0410 21:38:40.202230    4927 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I0410 21:38:40.202242    4927 node_conditions.go:123] node cpu capacity is 5
I0410 21:38:40.202252    4927 node_conditions.go:105] duration metric: took 3.827625ms to run NodePressure ...
I0410 21:38:40.202265    4927 ssh_runner.go:152] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.22.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0410 21:38:40.317036    4927 ssh_runner.go:152] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0410 21:38:40.325084    4927 ops.go:34] apiserver oom_adj: -16
I0410 21:38:40.325096    4927 kubeadm.go:604] restartCluster took 10.066309166s
I0410 21:38:40.325111    4927 kubeadm.go:392] StartCluster complete in 10.099533458s
I0410 21:38:40.325127    4927 settings.go:142] acquiring lock: {Name:mk9eede9db94f91642a591dd99004c280e5dcb3a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0410 21:38:40.325452    4927 settings.go:150] Updating kubeconfig:  /Users/pinkyjain/.kube/config
I0410 21:38:40.327151    4927 lock.go:35] WriteFile acquiring /Users/pinkyjain/.kube/config: {Name:mkd0a86ddd80edb0de0c54e4931456eaa58dda2e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0410 21:38:40.337371    4927 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I0410 21:38:40.337436    4927 start.go:229] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.22.3 ControlPlane:true Worker:true}
I0410 21:38:40.357363    4927 out.go:176] üîé  Verifying Kubernetes components...
I0410 21:38:40.337469    4927 ssh_runner.go:152] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.22.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0410 21:38:40.357505    4927 ssh_runner.go:152] Run: sudo systemctl is-active --quiet service kubelet
I0410 21:38:40.337812    4927 config.go:176] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.22.3
I0410 21:38:40.337986    4927 addons.go:415] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false helm-tiller:false ingress:true ingress-dns:true istio:false istio-provisioner:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I0410 21:38:40.357617    4927 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I0410 21:38:40.357633    4927 addons.go:153] Setting addon storage-provisioner=true in "minikube"
W0410 21:38:40.357640    4927 addons.go:165] addon storage-provisioner should already be in state true
I0410 21:38:40.357667    4927 host.go:66] Checking if "minikube" exists ...
I0410 21:38:40.357730    4927 addons.go:65] Setting ingress=true in profile "minikube"
I0410 21:38:40.357741    4927 addons.go:153] Setting addon ingress=true in "minikube"
W0410 21:38:40.357747    4927 addons.go:165] addon ingress should already be in state true
I0410 21:38:40.357777    4927 host.go:66] Checking if "minikube" exists ...
I0410 21:38:40.357842    4927 addons.go:65] Setting ingress-dns=true in profile "minikube"
I0410 21:38:40.357863    4927 addons.go:153] Setting addon ingress-dns=true in "minikube"
W0410 21:38:40.357869    4927 addons.go:165] addon ingress-dns should already be in state true
I0410 21:38:40.357899    4927 host.go:66] Checking if "minikube" exists ...
I0410 21:38:40.357952    4927 addons.go:65] Setting default-storageclass=true in profile "minikube"
I0410 21:38:40.357964    4927 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0410 21:38:40.360026    4927 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0410 21:38:40.364747    4927 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0410 21:38:40.366453    4927 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0410 21:38:40.367920    4927 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0410 21:38:40.516039    4927 out.go:176] üí°  After the addon is enabled, please run "minikube tunnel" and your ingress resources would be available at "127.0.0.1"
I0410 21:38:40.586228    4927 out.go:176]     ‚ñ™ Using image k8s.gcr.io/ingress-nginx/controller:v1.0.4
I0410 21:38:40.621629    4927 out.go:176]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0410 21:38:40.527257    4927 addons.go:153] Setting addon default-storageclass=true in "minikube"
W0410 21:38:40.621671    4927 addons.go:165] addon default-storageclass should already be in state true
I0410 21:38:40.551199    4927 out.go:176] üí°  After the addon is enabled, please run "minikube tunnel" and your ingress resources would be available at "127.0.0.1"
I0410 21:38:40.621693    4927 host.go:66] Checking if "minikube" exists ...
I0410 21:38:40.657086    4927 out.go:176]     ‚ñ™ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1
I0410 21:38:40.621703    4927 addons.go:348] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0410 21:38:40.657142    4927 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0410 21:38:40.622004    4927 cli_runner.go:115] Run: docker container inspect minikube --format={{.State.Status}}
I0410 21:38:40.693171    4927 out.go:176]     ‚ñ™ Using image gcr.io/k8s-minikube/minikube-ingress-dns:0.0.2
I0410 21:38:40.657232    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:40.730307    4927 out.go:176]     ‚ñ™ Using image k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1
I0410 21:38:40.693226    4927 addons.go:348] installing /etc/kubernetes/addons/ingress-dns-pod.yaml
I0410 21:38:40.730430    4927 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/ingress-dns-pod.yaml (2442 bytes)
I0410 21:38:40.731118    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:40.731147    4927 addons.go:348] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0410 21:38:40.731150    4927 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (17469 bytes)
I0410 21:38:40.731222    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:40.813043    4927 addons.go:348] installing /etc/kubernetes/addons/storageclass.yaml
I0410 21:38:40.813057    4927 ssh_runner.go:319] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0410 21:38:40.813117    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0410 21:38:40.855231    4927 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49546 SSHKeyPath:/Users/pinkyjain/.minikube/machines/minikube/id_rsa Username:docker}
I0410 21:38:40.857144    4927 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49546 SSHKeyPath:/Users/pinkyjain/.minikube/machines/minikube/id_rsa Username:docker}
I0410 21:38:40.857187    4927 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49546 SSHKeyPath:/Users/pinkyjain/.minikube/machines/minikube/id_rsa Username:docker}
I0410 21:38:40.881317    4927 cli_runner.go:115] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0410 21:38:40.882289    4927 start.go:719] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0410 21:38:40.936685    4927 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49546 SSHKeyPath:/Users/pinkyjain/.minikube/machines/minikube/id_rsa Username:docker}
I0410 21:38:40.951175    4927 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-dns-pod.yaml
I0410 21:38:40.955342    4927 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0410 21:38:40.969739    4927 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0410 21:38:40.995987    4927 api_server.go:51] waiting for apiserver process to appear ...
I0410 21:38:40.996060    4927 ssh_runner.go:152] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0410 21:38:41.091132    4927 ssh_runner.go:152] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.22.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0410 21:38:41.257230    4927 api_server.go:71] duration metric: took 919.768917ms to wait for apiserver process to appear ...
I0410 21:38:41.257252    4927 api_server.go:87] waiting for apiserver healthz status ...
I0410 21:38:41.257262    4927 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:49550/healthz ...
I0410 21:38:41.257267    4927 addons.go:386] Verifying addon ingress=true in "minikube"
I0410 21:38:41.304323    4927 out.go:176] üîé  Verifying ingress addon...
I0410 21:38:41.264519    4927 api_server.go:266] https://127.0.0.1:49550/healthz returned 200:
ok
I0410 21:38:41.313406    4927 api_server.go:140] control plane version: v1.22.3
I0410 21:38:41.313426    4927 api_server.go:130] duration metric: took 56.166375ms to wait for apiserver health ...
I0410 21:38:41.313439    4927 system_pods.go:43] waiting for kube-system pods to appear ...
I0410 21:38:41.313893    4927 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0410 21:38:41.316906    4927 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0410 21:38:41.316912    4927 kapi.go:108] duration metric: took 3.023709ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0410 21:38:41.319098    4927 system_pods.go:59] 8 kube-system pods found
I0410 21:38:41.319104    4927 system_pods.go:61] "coredns-78fcd69978-m2pqh" [141b0481-6d98-41bc-a7ac-a3751fe55978] Running
I0410 21:38:41.319107    4927 system_pods.go:61] "etcd-minikube" [79d53611-8f0a-4b93-a5f6-02bc73cb1d9e] Running
I0410 21:38:41.319119    4927 system_pods.go:61] "kube-apiserver-minikube" [b64312e9-1e75-4419-971a-b4a5d6153235] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0410 21:38:41.319121    4927 system_pods.go:61] "kube-controller-manager-minikube" [6d0ed9a4-d50b-4aa7-af02-9418418138e1] Running
I0410 21:38:41.319123    4927 system_pods.go:61] "kube-ingress-dns-minikube" [e75af6ad-ff32-4304-9f8c-8528d04c7a08] Running
I0410 21:38:41.319125    4927 system_pods.go:61] "kube-proxy-kzwvq" [25752949-94fa-4a25-b668-e2ea8f56cbcf] Running
I0410 21:38:41.319127    4927 system_pods.go:61] "kube-scheduler-minikube" [a5f70102-3807-4672-aa82-c91fbf8d4e15] Running
I0410 21:38:41.319128    4927 system_pods.go:61] "storage-provisioner" [102b0da6-9771-45cc-936b-7b42d2b17803] Running
I0410 21:38:41.319130    4927 system_pods.go:74] duration metric: took 5.687042ms to wait for pod list to return data ...
I0410 21:38:41.319133    4927 kubeadm.go:547] duration metric: took 981.680834ms to wait for : map[apiserver:true system_pods:true] ...
I0410 21:38:41.319140    4927 node_conditions.go:102] verifying NodePressure condition ...
I0410 21:38:41.321076    4927 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I0410 21:38:41.321081    4927 node_conditions.go:123] node cpu capacity is 5
I0410 21:38:41.321086    4927 node_conditions.go:105] duration metric: took 1.944834ms to run NodePressure ...
I0410 21:38:41.321091    4927 start.go:234] waiting for startup goroutines ...
I0410 21:38:41.385466    4927 out.go:176] üåü  Enabled addons: ingress-dns, storage-provisioner, ingress, default-storageclass
I0410 21:38:41.385530    4927 addons.go:417] enableAddons completed in 1.047608708s
I0410 21:38:41.434173    4927 start.go:473] kubectl: 1.22.5, cluster: 1.22.3 (minor skew: 0)
I0410 21:38:41.453537    4927 out.go:176] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Sun 2022-04-10 13:38:26 UTC, end at Sun 2022-04-10 13:52:21 UTC. --
Apr 10 13:38:26 minikube systemd[1]: Starting Docker Application Container Engine...
Apr 10 13:38:26 minikube dockerd[129]: time="2022-04-10T13:38:26.790639049Z" level=info msg="Starting up"
Apr 10 13:38:26 minikube dockerd[129]: time="2022-04-10T13:38:26.794732840Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Apr 10 13:38:26 minikube dockerd[129]: time="2022-04-10T13:38:26.794763549Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Apr 10 13:38:26 minikube dockerd[129]: time="2022-04-10T13:38:26.794778965Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Apr 10 13:38:26 minikube dockerd[129]: time="2022-04-10T13:38:26.794787215Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Apr 10 13:38:26 minikube dockerd[129]: time="2022-04-10T13:38:26.798190340Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Apr 10 13:38:26 minikube dockerd[129]: time="2022-04-10T13:38:26.798211007Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Apr 10 13:38:26 minikube dockerd[129]: time="2022-04-10T13:38:26.798222840Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Apr 10 13:38:26 minikube dockerd[129]: time="2022-04-10T13:38:26.798228007Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Apr 10 13:38:26 minikube dockerd[129]: time="2022-04-10T13:38:26.806702465Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Apr 10 13:38:26 minikube dockerd[129]: time="2022-04-10T13:38:26.954289382Z" level=info msg="Loading containers: start."
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.244147674Z" level=info msg="Removing stale sandbox 3fd946b8cea19140cc28080248d31c3ea0f01dd579ba5fff9ce54a67b4282754 (8f93b028370a43a4f4e85407b8bcacf26cf2b2335c320d4fa2a3af89eec75e49)"
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.246164299Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint dcf7709d8e07c42bee1235d500a3baf8f0055b3a0f1c79ef842278e52a22fef9 31d16c778f4d415e3dc560897b53e8d7464897f09212ba4a380c0b23529df42d], retrying...."
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.308497799Z" level=info msg="Removing stale sandbox 4fc458864c61a5294ee12d2c749af7637b894232e80c24f0c91326b742f0b0ff (8e47e9dcc299a7d602693a8d2fb5796e6e8d90480550899f14877e0715b875ab)"
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.312159799Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 3aad9067b52494e06281fbe8aba63280da58ffc388287360b91ca26652d5f3c6 6ef2f6b1decb40a62f72c16184fe3ae00be345aee484e2162751773a88056d5e], retrying...."
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.379121049Z" level=info msg="Removing stale sandbox 866f85f36e1d9a6af151fd06fcba77acda965884ec914d6268272f9e5533cd00 (9ac4ec4ee1fb3e4a0d1a9f5c0828b72482c7e5c8ebf4058e879b26ea57c16c3e)"
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.380403091Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint dcf7709d8e07c42bee1235d500a3baf8f0055b3a0f1c79ef842278e52a22fef9 ce154a711e6cc85a66b8e5e4000c4aaa26e8656d6a496e2d988efc787742e7db], retrying...."
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.448045591Z" level=info msg="Removing stale sandbox 8b2b232033551a9014dc6333cf74dc7afd661bdcb539477581e80960f1fd5589 (dde9f307f4828cbc9eb506469462378889925033805be808f4abd4c01b5e2941)"
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.449319466Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint dcf7709d8e07c42bee1235d500a3baf8f0055b3a0f1c79ef842278e52a22fef9 14daf7d3ad5c6fae39ab8c951da590962e69ee150c92573567166ca624721ac6], retrying...."
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.514852174Z" level=info msg="Removing stale sandbox ce8fb14bc1aaa1abc9add7baf5f4d148b93322ca543fdbd4728e43b29fc7107a (fe345545e97daa9192d6d365e58a1b2b629c70190ff23ab84cbd614e39cd610b)"
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.518725757Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 3aad9067b52494e06281fbe8aba63280da58ffc388287360b91ca26652d5f3c6 294ae34fa49d6b4e2c918cd3a8a03b4d3e8b810538418a0247543240328dd59d], retrying...."
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.589323091Z" level=info msg="Removing stale sandbox 07393c9d8ff8b47c7629aaed1b2323e0a946c2f19d1358a15aaf939370ea36cb (82098bdc6c64507a3e8760eaa01d4d98c9d706a79ad298a9fc369569b114ab79)"
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.592416841Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 3aad9067b52494e06281fbe8aba63280da58ffc388287360b91ca26652d5f3c6 83a16a48f3351443442dfe5889b9afc446e4ca6895279dd6e07259938163559f], retrying...."
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.661431716Z" level=info msg="Removing stale sandbox 12e5a319824beb9c35cedb2fafcaaab91e26d909643274ab60f0dcbc5cc3cd91 (1af73714b0b61ffc2208141ac4994b4cf0441484af55146cb9fb8bca50f58fe9)"
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.666199174Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint dcf7709d8e07c42bee1235d500a3baf8f0055b3a0f1c79ef842278e52a22fef9 60fec230e4f95915b099fe95316ee80ff1c2066daa9268ce9781a285cb18bd61], retrying...."
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.728849466Z" level=info msg="Removing stale sandbox 6e22307c9fe47ec8e31d6498c694ff9da0c6f70d849efa0607a14986af78a70c (f97a239bcce5255603bcd48dad74f230809ac71e527a85520cd378fb6bed15a3)"
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.729975466Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint dcf7709d8e07c42bee1235d500a3baf8f0055b3a0f1c79ef842278e52a22fef9 c7c4ba357c36225d0cbc6a6f7c02d49b7659e3f166b441e5db03cfdb3316eca2], retrying...."
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.800555174Z" level=info msg="Removing stale sandbox 80234211b48aa8e3332811ded1dd5e761b441eab8236e3aebb7a6f92f7c10818 (f862c15a5f41f6f98d6c921bf51fd0a41b497780281544dd9848c07411a0b162)"
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.802179758Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint dcf7709d8e07c42bee1235d500a3baf8f0055b3a0f1c79ef842278e52a22fef9 2a2849006e9c64ea86a9067f1875f47cdb63757a5b3d02677aa0977fe4a88fce], retrying...."
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.866971299Z" level=info msg="Removing stale sandbox 8d106d4b134787b6874df2b07d44d8fe9e0a3d4b3eb54125612975ad14e89ad7 (e68ca9b0c8a19901da384eb70ebd05a9603d69aa0dee0be132d4b69fd1ac15db)"
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.870749799Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 3aad9067b52494e06281fbe8aba63280da58ffc388287360b91ca26652d5f3c6 16637ec0b28ff9272950b03a30fcd645f582fa470c27711f5cbc43414ad7987f], retrying...."
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.936035966Z" level=info msg="Removing stale sandbox d3a5b26ebb59557d4ab62d0f6ce96b54762fe24ba9e9d18c32d4902ec28b4c54 (ceb5fde1fd16659bb977627780c963e54b3b573a5993f4b74fba627e6ea2f987)"
Apr 10 13:38:27 minikube dockerd[129]: time="2022-04-10T13:38:27.952514258Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 3aad9067b52494e06281fbe8aba63280da58ffc388287360b91ca26652d5f3c6 da911719df0fd6ebda69cde6bae643e12395794b18bbdd7a512e73e54a4706b5], retrying...."
Apr 10 13:38:28 minikube dockerd[129]: time="2022-04-10T13:38:28.015789591Z" level=info msg="Removing stale sandbox dc9e80612560c96045a9ab8e24f11cf62731bad38ee9ace61f3512767d6c78d3 (354eecbfc005d73dc377dc539acef62420d54b20a05aa3bbbc7dd139ca5a0e42)"
Apr 10 13:38:28 minikube dockerd[129]: time="2022-04-10T13:38:28.017189841Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint dcf7709d8e07c42bee1235d500a3baf8f0055b3a0f1c79ef842278e52a22fef9 db4d5e9d79bfdaa8d094b85ffa1a3c6e4cbe1877f6111343d7535a5bdaf444a3], retrying...."
Apr 10 13:38:28 minikube dockerd[129]: time="2022-04-10T13:38:28.038403383Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Apr 10 13:38:28 minikube dockerd[129]: time="2022-04-10T13:38:28.088090508Z" level=info msg="Loading containers: done."
Apr 10 13:38:28 minikube dockerd[129]: time="2022-04-10T13:38:28.125151924Z" level=info msg="Docker daemon" commit=75249d8 graphdriver(s)=overlay2 version=20.10.8
Apr 10 13:38:28 minikube dockerd[129]: time="2022-04-10T13:38:28.125473758Z" level=info msg="Daemon has completed initialization"
Apr 10 13:38:28 minikube systemd[1]: Started Docker Application Container Engine.
Apr 10 13:38:28 minikube dockerd[129]: time="2022-04-10T13:38:28.149065633Z" level=info msg="API listen on [::]:2376"
Apr 10 13:38:28 minikube dockerd[129]: time="2022-04-10T13:38:28.151681758Z" level=info msg="API listen on /var/run/docker.sock"
Apr 10 13:38:41 minikube dockerd[129]: time="2022-04-10T13:38:41.888516500Z" level=warning msg="Published ports are discarded when using host network mode"
Apr 10 13:38:41 minikube dockerd[129]: time="2022-04-10T13:38:41.912030542Z" level=warning msg="Published ports are discarded when using host network mode"
Apr 10 13:39:05 minikube dockerd[129]: time="2022-04-10T13:39:05.376209386Z" level=info msg="ignoring event" container=857a4c0428f5ed9c090d3185bd469ef4e8e58a1b6dd83bbeb6bbb0581635defe module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 10 13:39:11 minikube dockerd[129]: time="2022-04-10T13:39:11.652474292Z" level=info msg="ignoring event" container=306e5b7632579f4285dc977eee0bacb953848f83fdd21d2e9e0f7bf8f66ec83d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 10 13:39:35 minikube dockerd[129]: time="2022-04-10T13:39:35.404644178Z" level=info msg="ignoring event" container=7a163613d6eb7dd9cc46b02b06c5bc2218c6efaa17fe9a535b2e69791112fe22 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 10 13:40:05 minikube dockerd[129]: time="2022-04-10T13:40:05.383655761Z" level=info msg="ignoring event" container=4c369aa019641d05b532e007586242215b7cc00a0212c57c9cbb10ada29e16b1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 10 13:40:39 minikube dockerd[129]: time="2022-04-10T13:40:39.892131263Z" level=info msg="ignoring event" container=67195c335e2a15bb7996d59ca237b77d185a98514d87d6909faa61b68f0ffd94 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 10 13:41:09 minikube dockerd[129]: time="2022-04-10T13:41:09.922269388Z" level=info msg="ignoring event" container=22f2e76663e8cb46c68d479e4c08a807551b426c9316264d4b038079c13d33de module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 10 13:41:39 minikube dockerd[129]: time="2022-04-10T13:41:39.922954972Z" level=info msg="ignoring event" container=bc3f91c57decf3821ba9681f3916ddec2875d893f4a5173fdfca987bb0ca9618 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 10 13:42:09 minikube dockerd[129]: time="2022-04-10T13:42:09.931579180Z" level=info msg="ignoring event" container=87f791b940a26f125d1adec483d143d802aa1690063437a8b17665882b8b29a1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 10 13:42:39 minikube dockerd[129]: time="2022-04-10T13:42:39.924357888Z" level=info msg="ignoring event" container=0ed68e84009241e5e27c4d6ceeb24b0aef8a881b48b40c35a020ed8d98a9a91f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 10 13:43:09 minikube dockerd[129]: time="2022-04-10T13:43:09.922059722Z" level=info msg="ignoring event" container=90028b274f61d013acb0558e98ef50c4baaf8d19d1eee01b366b3562eb04d5be module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 10 13:44:19 minikube dockerd[129]: time="2022-04-10T13:44:19.925413920Z" level=info msg="ignoring event" container=f41f9a9842b81ae680f02933b9dd8954fd18b863da28ea7a7113d8a0ed6f10e3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 10 13:46:09 minikube dockerd[129]: time="2022-04-10T13:46:09.920875763Z" level=info msg="ignoring event" container=3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 10 13:49:19 minikube dockerd[129]: time="2022-04-10T13:49:19.929108712Z" level=info msg="ignoring event" container=2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                      ATTEMPT             POD ID
2f4410795b334       3052c0fbe0a2e                                                                   3 minutes ago       Exited              helm-chart-nodeapp        7                   f6cc1886d5986
a22c0402f828e       66749159455b3                                                                   12 minutes ago      Running             storage-provisioner       11                  1c81a745cd291
4bba3328ac05b       mongo@sha256:fed6248ae0bb0d54c0448eb786c87120737eedc522172ee1536ad47789782348   13 minutes ago      Running             mongodb                   1                   199cc0940d591
5527601af3f3f       d918321a60055                                                                   13 minutes ago      Running             controller                3                   0932c8ce84e52
eefb9eee263d5       3a8d1d04758e2                                                                   13 minutes ago      Running             kube-proxy                5                   cfcbf712a047a
7855efffff143       1499ed4fbd0aa                                                                   13 minutes ago      Running             minikube-ingress-dns      3                   182c97f4d6ac5
0368f7024d623       46c83b2e09c5f                                                                   13 minutes ago      Running             helm-chart-nodeapp        1                   2c97bc699bbd8
79a9f7b5111bb       008e44c427c6f                                                                   13 minutes ago      Running             coredns                   5                   f14c4a8afdfdd
306e5b7632579       66749159455b3                                                                   13 minutes ago      Exited              storage-provisioner       10                  1c81a745cd291
c54974745f29a       32513be2649f4                                                                   13 minutes ago      Running             kube-apiserver            5                   8b4eb40a6891e
7bad8e5411256       3893bb7d23934                                                                   13 minutes ago      Running             kube-scheduler            5                   ee213e51eea18
fb95b9777a829       a2ee49d2d4320                                                                   13 minutes ago      Running             etcd                      5                   d8cb025c9276e
fbce405c00582       42e51ba6db03e                                                                   13 minutes ago      Running             kube-controller-manager   5                   a94349f77bbc3
d198a466b5e14       46c83b2e09c5f                                                                   7 hours ago         Exited              helm-chart-nodeapp        0                   e68ca9b0c8a19
1a0b068d2f392       mongo@sha256:fed6248ae0bb0d54c0448eb786c87120737eedc522172ee1536ad47789782348   8 hours ago         Exited              mongodb                   0                   fe345545e97da
7a0111e619b0b       3a8d1d04758e2                                                                   29 hours ago        Exited              kube-proxy                4                   9ac4ec4ee1fb3
d39728cb18391       d918321a60055                                                                   29 hours ago        Exited              controller                2                   ceb5fde1fd166
d66e2b3f1b71b       1499ed4fbd0aa                                                                   29 hours ago        Exited              minikube-ingress-dns      2                   8f93b028370a4
aadf0cc9aad4c       008e44c427c6f                                                                   29 hours ago        Exited              coredns                   4                   8e47e9dcc299a
b33da1b1e8204       a2ee49d2d4320                                                                   29 hours ago        Exited              etcd                      4                   f97a239bcce52
596b935cd69fc       3893bb7d23934                                                                   29 hours ago        Exited              kube-scheduler            4                   1af73714b0b61
babd5250bcccf       42e51ba6db03e                                                                   29 hours ago        Exited              kube-controller-manager   4                   f862c15a5f41f
4ef528135a5e6       32513be2649f4                                                                   29 hours ago        Exited              kube-apiserver            4                   dde9f307f4828

* 
* ==> coredns [79a9f7b5111b] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.4
linux/arm64, go1.16.4, 053c4d5
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> coredns [aadf0cc9aad4] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.4
linux/arm64, go1.16.4, 053c4d5
[INFO] plugin/ready: Still waiting on: "kubernetes"
[ERROR] plugin/errors: 2 2944854969682463511.7879902740572061534. HINFO: read udp 172.17.0.4:38270->192.168.65.2:53: i/o timeout
[ERROR] plugin/errors: 2 2944854969682463511.7879902740572061534. HINFO: read udp 172.17.0.4:42798->192.168.65.2:53: i/o timeout
[ERROR] plugin/errors: 2 2944854969682463511.7879902740572061534. HINFO: read udp 172.17.0.4:39888->192.168.65.2:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[ERROR] plugin/errors: 2 2944854969682463511.7879902740572061534. HINFO: read udp 172.17.0.4:48964->192.168.65.2:53: i/o timeout
[ERROR] plugin/errors: 2 2944854969682463511.7879902740572061534. HINFO: read udp 172.17.0.4:50223->192.168.65.2:53: i/o timeout
[ERROR] plugin/errors: 2 2944854969682463511.7879902740572061534. HINFO: read udp 172.17.0.4:50434->192.168.65.2:53: i/o timeout
[ERROR] plugin/errors: 2 2944854969682463511.7879902740572061534. HINFO: read udp 172.17.0.4:47847->192.168.65.2:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[ERROR] plugin/errors: 2 2944854969682463511.7879902740572061534. HINFO: read udp 172.17.0.4:33419->192.168.65.2:53: i/o timeout
[ERROR] plugin/errors: 2 2944854969682463511.7879902740572061534. HINFO: read udp 172.17.0.4:56498->192.168.65.2:53: i/o timeout
[ERROR] plugin/errors: 2 2944854969682463511.7879902740572061534. HINFO: read udp 172.17.0.4:54812->192.168.65.2:53: i/o timeout

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane,master
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=76b94fb3c4e8ac5062daf70d60cf03ddcc0a741b
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/updated_at=2022_01_23T17_52_30_0700
                    minikube.k8s.io/version=v1.24.0
                    node-role.kubernetes.io/control-plane=
                    node-role.kubernetes.io/master=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 23 Jan 2022 09:52:27 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 10 Apr 2022 13:52:16 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 10 Apr 2022 13:48:41 +0000   Sun, 23 Jan 2022 09:52:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 10 Apr 2022 13:48:41 +0000   Sun, 23 Jan 2022 09:52:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 10 Apr 2022 13:48:41 +0000   Sun, 23 Jan 2022 09:52:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 10 Apr 2022 13:48:41 +0000   Sun, 23 Jan 2022 09:52:40 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                5
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             24587364Ki
  pods:               110
Allocatable:
  cpu:                5
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             24587364Ki
  pods:               110
System Info:
  Machine ID:                 b4bce83d7afb4708a4b1b0614c6c2ef8
  System UUID:                b4bce83d7afb4708a4b1b0614c6c2ef8
  Boot ID:                    58045e93-69e9-48b6-baeb-4ed2739c579a
  Kernel Version:             5.10.104-linuxkit
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://20.10.8
  Kubelet Version:            v1.22.3
  Kube-Proxy Version:         v1.22.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     aa-helm-chart-nodeapp-8d9c79648-2c84j        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7h6m
  default                     bb-helm-chart-nodeapp-f466cbff5-jdddc        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11m
  default                     mongodb-deployment-7544874b58-vgbp5          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7h35m
  ingress-nginx               ingress-nginx-controller-5f66978484-ns7qb    100m (2%!)(MISSING)     0 (0%!)(MISSING)      90Mi (0%!)(MISSING)        0 (0%!)(MISSING)         6d9h
  kube-system                 coredns-78fcd69978-m2pqh                     100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (0%!)(MISSING)     77d
  kube-system                 etcd-minikube                                100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         77d
  kube-system                 kube-apiserver-minikube                      250m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         77d
  kube-system                 kube-controller-manager-minikube             200m (4%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         77d
  kube-system                 kube-ingress-dns-minikube                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d8h
  kube-system                 kube-proxy-kzwvq                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         77d
  kube-system                 kube-scheduler-minikube                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         77d
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         77d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (17%!)(MISSING)  0 (0%!)(MISSING)
  memory             260Mi (1%!)(MISSING)  170Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From        Message
  ----    ------                   ----               ----        -------
  Normal  Starting                 13m                kube-proxy  
  Normal  Starting                 13m                kubelet     Starting kubelet.
  Normal  NodeHasSufficientMemory  13m (x8 over 13m)  kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    13m (x8 over 13m)  kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     13m (x7 over 13m)  kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  13m                kubelet     Updated Node Allocatable limit across pods

* 
* ==> dmesg <==
* [Apr10 13:33] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.034151] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[  +5.711222] grpcfuse: loading out-of-tree module taints kernel.

* 
* ==> etcd [b33da1b1e820] <==
* 2022-04-10 08:32:34.557798 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:32:44.556996 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:32:54.557245 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:33:04.557267 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:33:14.556597 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:33:24.556857 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:33:34.557578 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:33:44.556647 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:33:54.556463 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:34:04.556194 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:34:14.556015 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:34:24.555565 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:34:34.555972 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:34:44.555971 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:34:54.555455 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:35:04.555138 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:35:14.555170 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:35:24.559904 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:35:34.554894 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:35:44.554874 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:35:54.554511 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:36:04.554505 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:36:14.554467 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:36:16.564115 I | mvcc: store.index: compact 124361
2022-04-10 08:36:16.564589 I | mvcc: finished scheduled compaction at 124361 (took 381.375¬µs)
2022-04-10 08:36:24.553747 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:36:34.553825 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:36:44.553364 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:36:54.553241 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:37:04.553550 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:37:14.552745 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:37:24.552898 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:37:34.552702 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:37:44.552088 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:37:54.552034 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:38:04.552970 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:38:14.551460 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:38:24.551428 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:38:34.552654 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:38:44.551666 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:38:54.550945 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:39:04.551533 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:39:14.551084 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:39:24.550552 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:39:34.551008 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:39:44.550468 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:39:54.550045 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:40:04.551336 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:40:14.549639 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:40:24.549499 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:40:34.549512 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:40:44.548916 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:40:54.549181 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:41:04.549452 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:41:14.548672 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:41:16.562748 I | mvcc: store.index: compact 124614
2022-04-10 08:41:16.563229 I | mvcc: finished scheduled compaction at 124614 (took 387.875¬µs)
2022-04-10 08:41:24.548740 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:41:34.548637 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 08:41:44.548265 I | etcdserver/api/etcdhttp: /health OK (status code 200)

* 
* ==> etcd [fb95b9777a82] <==
* 2022-04-10 13:42:49.219916 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:42:59.220655 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:43:09.219922 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:43:19.220111 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:43:29.219771 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:43:39.221505 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:43:49.220025 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:43:59.220454 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:44:09.220805 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:44:19.220218 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:44:29.219989 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:44:39.220899 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:44:49.220012 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:44:59.220062 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:45:09.219884 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:45:19.220297 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:45:29.220434 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:45:39.220148 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:45:49.220568 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:45:59.220673 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:46:09.219923 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:46:19.220736 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:46:29.219920 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:46:39.220603 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:46:49.220654 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:46:59.220376 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:47:09.219988 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:47:19.220984 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:47:29.220799 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:47:39.220212 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:47:49.219705 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:47:59.220767 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:48:09.220458 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:48:19.219881 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:48:29.220391 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:48:37.290796 I | mvcc: store.index: compact 125364
2022-04-10 13:48:37.302753 I | mvcc: finished scheduled compaction at 125364 (took 11.837625ms)
2022-04-10 13:48:39.219922 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:48:49.220729 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:48:59.220067 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:49:09.220994 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:49:19.219961 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:49:29.220403 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:49:39.219920 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:49:49.220481 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:49:59.220380 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:50:09.220655 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:50:19.220492 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:50:29.219527 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:50:39.221541 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:50:49.219885 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:50:59.220627 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:51:09.260999 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:51:19.219728 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:51:29.219938 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:51:39.220479 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:51:49.221832 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:51:59.220121 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:52:09.219725 I | etcdserver/api/etcdhttp: /health OK (status code 200)
2022-04-10 13:52:19.220185 I | etcdserver/api/etcdhttp: /health OK (status code 200)

* 
* ==> kernel <==
*  13:52:22 up 18 min,  0 users,  load average: 1.62, 1.96, 2.12
Linux minikube 5.10.104-linuxkit #1 SMP PREEMPT Wed Mar 9 19:01:25 UTC 2022 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.2 LTS"

* 
* ==> kube-apiserver [4ef528135a5e] <==
* E0409 16:36:59.345211       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0409 16:36:59.345377       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0409 19:28:02.169519       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0409 19:28:02.169842       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0409 22:35:24.400692       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0409 22:35:24.401115       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0410 00:52:41.845267       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0410 01:01:44.796931       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0410 01:01:44.797296       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0410 02:03:17.649699       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0410 02:03:17.649847       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0410 03:31:53.265785       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0410 03:31:53.265785       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0410 04:51:22.626428       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
E0410 04:51:22.626862       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, Token has expired.]"
I0410 06:08:51.476890       1 trace.go:205] Trace[1674328045]: "Get" url:/api/v1/namespaces/default/pods/dd-helm-chart-nodeapp-788f4c77bd-pwhzw/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:0c52e00d-344c-4e6e-9485-dde5225d3713,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 06:08:40.241) (total time: 11235ms):
Trace[1674328045]: ---"Transformed response object" 11233ms (06:08:51.476)
Trace[1674328045]: [11.235088839s] [11.235088839s] END
I0410 06:10:29.977133       1 trace.go:205] Trace[892953343]: "Get" url:/api/v1/namespaces/default/pods/dd-helm-chart-nodeapp-788f4c77bd-569lw/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:95d3b65a-b7f3-4880-a9e1-f2bd9e23d41e,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 06:10:27.784) (total time: 2192ms):
Trace[892953343]: ---"Transformed response object" 2191ms (06:10:29.977)
Trace[892953343]: [2.192465626s] [2.192465626s] END
I0410 06:12:56.260754       1 trace.go:205] Trace[201751746]: "Get" url:/api/v1/namespaces/default/pods/dd-helm-chart-nodeapp-788f4c77bd-9p2zk/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:4c107ef0-0fe2-4c60-b165-917e68c43711,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 06:12:53.568) (total time: 2692ms):
Trace[201751746]: ---"Transformed response object" 2690ms (06:12:56.260)
Trace[201751746]: [2.692211709s] [2.692211709s] END
I0410 06:13:22.770946       1 trace.go:205] Trace[800392676]: "Get" url:/api/v1/namespaces/default/pods/aa-helm-chart-nodeapp-97d6fc644-b86ws/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:1e0553e9-9e26-4013-a713-55e37c39ab9b,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 06:13:18.643) (total time: 4127ms):
Trace[800392676]: ---"Transformed response object" 4126ms (06:13:22.770)
Trace[800392676]: [4.127887335s] [4.127887335s] END
I0410 06:15:44.576244       1 trace.go:205] Trace[2088605336]: "Get" url:/api/v1/namespaces/default/pods/aa-helm-chart-nodeapp-97d6fc644-c8d2b/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:c3eec8ba-dc49-483c-b0c4-5be7e34cd1f5,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 06:15:36.830) (total time: 7745ms):
Trace[2088605336]: ---"Transformed response object" 7744ms (06:15:44.576)
Trace[2088605336]: [7.745879253s] [7.745879253s] END
I0410 06:25:18.300418       1 trace.go:205] Trace[1589334971]: "Get" url:/api/v1/namespaces/default/pods/aa-helm-chart-nodeapp-97d6fc644-bqk8f/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:41c1e8a0-6f52-4ffd-82d8-5c8caf29b47a,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 06:25:07.009) (total time: 11290ms):
Trace[1589334971]: ---"Transformed response object" 11289ms (06:25:18.300)
Trace[1589334971]: [11.290792922s] [11.290792922s] END
I0410 06:27:16.956073       1 trace.go:205] Trace[1069695336]: "Get" url:/api/v1/namespaces/default/pods/aa-helm-chart-nodeapp-97d6fc644-bqk8f/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:8863c1ca-5354-4909-995c-609a2b06af1c,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 06:27:07.526) (total time: 9429ms):
Trace[1069695336]: ---"Transformed response object" 9427ms (06:27:16.956)
Trace[1069695336]: [9.429222588s] [9.429222588s] END
I0410 06:30:10.166158       1 trace.go:205] Trace[1303844342]: "Get" url:/api/v1/namespaces/default/pods/aa-helm-chart-nodeapp-8d9c79648-86h5h/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:0aa0890f-4c9a-496c-a231-29f914b18994,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 06:30:07.001) (total time: 3164ms):
Trace[1303844342]: ---"Transformed response object" 3163ms (06:30:10.166)
Trace[1303844342]: [3.164543501s] [3.164543501s] END
I0410 06:40:05.429788       1 trace.go:205] Trace[401681402]: "Get" url:/api/v1/namespaces/default/pods/aa-helm-chart-nodeapp-8d9c79648-86h5h/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:73003a12-cda0-4eb8-90a1-a85a1fe2cf3c,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 06:33:56.375) (total time: 369060ms):
Trace[401681402]: ---"Transformed response object" 369059ms (06:40:05.429)
Trace[401681402]: [6m9.060262295s] [6m9.060262295s] END
W0410 06:44:05.993271       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
I0410 06:54:08.814167       1 trace.go:205] Trace[304978753]: "Get" url:/api/v1/namespaces/default/pods/aa-helm-chart-nodeapp-8d9c79648-2c84j/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:a411a629-1a7d-4fdc-bffe-a191e0aea3f4,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 06:53:51.880) (total time: 16933ms):
Trace[304978753]: ---"Transformed response object" 16932ms (06:54:08.814)
Trace[304978753]: [16.93367705s] [16.93367705s] END
W0410 06:57:54.041036       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0410 07:51:25.781935       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
I0410 07:53:00.643219       1 trace.go:205] Trace[108598057]: "Get" url:/api/v1/namespaces/default/pods/bb-helm-chart-nodeapp-f466cbff5-vjfj5/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:83b33011-d572-4815-a6f8-e1da72c67fcb,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 07:52:43.765) (total time: 16878ms):
Trace[108598057]: ---"Transformed response object" 16877ms (07:53:00.643)
Trace[108598057]: [16.878007924s] [16.878007924s] END
I0410 07:58:04.029340       1 trace.go:205] Trace[1546489614]: "Get" url:/api/v1/namespaces/default/pods/bb-helm-chart-nodeapp-f466cbff5-5rfxw/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:a3e357c8-e244-4bd8-a0ba-2fd2c1ab952c,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 07:58:02.036) (total time: 1992ms):
Trace[1546489614]: ---"Transformed response object" 1991ms (07:58:04.029)
Trace[1546489614]: [1.992542918s] [1.992542918s] END
I0410 08:01:30.817183       1 trace.go:205] Trace[275862563]: "Get" url:/api/v1/namespaces/default/pods/bb-helm-chart-nodeapp-f466cbff5-5rfxw/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:269755d6-a3fc-449c-8620-872c0e29bfeb,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 08:01:18.769) (total time: 12047ms):
Trace[275862563]: ---"Transformed response object" 12046ms (08:01:30.817)
Trace[275862563]: [12.047782923s] [12.047782923s] END
I0410 08:03:22.029192       1 trace.go:205] Trace[357833465]: "Get" url:/api/v1/namespaces/default/pods/bb-helm-chart-nodeapp-f466cbff5-5rfxw/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:2ab9fd94-eb97-48c4-97f3-44931deb0c5b,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 08:03:07.339) (total time: 14689ms):
Trace[357833465]: ---"Transformed response object" 14688ms (08:03:22.029)
Trace[357833465]: [14.689860673s] [14.689860673s] END

* 
* ==> kube-apiserver [c54974745f29] <==
* W0410 13:38:37.804112       1 genericapiserver.go:455] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0410 13:38:37.804129       1 genericapiserver.go:455] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
W0410 13:38:37.806900       1 genericapiserver.go:455] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
W0410 13:38:37.808429       1 genericapiserver.go:455] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
W0410 13:38:37.812398       1 genericapiserver.go:455] Skipping API apps/v1beta2 because it has no resources.
W0410 13:38:37.812413       1 genericapiserver.go:455] Skipping API apps/v1beta1 because it has no resources.
W0410 13:38:37.814038       1 genericapiserver.go:455] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
I0410 13:38:37.816517       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0410 13:38:37.816538       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W0410 13:38:37.831528       1 genericapiserver.go:455] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0410 13:38:38.916552       1 dynamic_cafile_content.go:155] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0410 13:38:38.916736       1 dynamic_cafile_content.go:155] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0410 13:38:38.916863       1 secure_serving.go:266] Serving securely on [::]:8443
I0410 13:38:38.916919       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0410 13:38:38.916910       1 dynamic_serving_content.go:129] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0410 13:38:38.917778       1 controller.go:83] Starting OpenAPI AggregationController
I0410 13:38:38.918452       1 apf_controller.go:312] Starting API Priority and Fairness config controller
I0410 13:38:38.919665       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0410 13:38:38.919681       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0410 13:38:38.919709       1 autoregister_controller.go:141] Starting autoregister controller
I0410 13:38:38.919720       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0410 13:38:38.919847       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I0410 13:38:38.921446       1 available_controller.go:491] Starting AvailableConditionController
I0410 13:38:38.921458       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0410 13:38:38.921809       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0410 13:38:38.921821       1 shared_informer.go:240] Waiting for caches to sync for cluster_authentication_trust_controller
I0410 13:38:38.922533       1 dynamic_serving_content.go:129] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0410 13:38:38.922554       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0410 13:38:38.922559       1 shared_informer.go:240] Waiting for caches to sync for crd-autoregister
I0410 13:38:38.922635       1 controller.go:85] Starting OpenAPI controller
I0410 13:38:38.922656       1 naming_controller.go:291] Starting NamingConditionController
I0410 13:38:38.922662       1 establishing_controller.go:76] Starting EstablishingController
I0410 13:38:38.923696       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0410 13:38:38.923712       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0410 13:38:38.923720       1 crd_finalizer.go:266] Starting CRDFinalizer
I0410 13:38:38.927827       1 dynamic_cafile_content.go:155] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0410 13:38:38.927863       1 dynamic_cafile_content.go:155] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
E0410 13:38:38.985438       1 controller.go:152] Unable to remove old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0410 13:38:39.018697       1 apf_controller.go:317] Running API Priority and Fairness config worker
I0410 13:38:39.019701       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0410 13:38:39.022892       1 cache.go:39] Caches are synced for autoregister controller
I0410 13:38:39.023103       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0410 13:38:39.023369       1 shared_informer.go:247] Caches are synced for cluster_authentication_trust_controller 
I0410 13:38:39.023442       1 shared_informer.go:247] Caches are synced for crd-autoregister 
I0410 13:38:39.056556       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I0410 13:38:39.063278       1 shared_informer.go:247] Caches are synced for node_authorizer 
I0410 13:38:39.917098       1 controller.go:132] OpenAPI AggregationController: action for item : Nothing (removed from the queue).
I0410 13:38:39.917119       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0410 13:38:39.923838       1 storage_scheduling.go:148] all system priority classes are created successfully or already exist.
I0410 13:38:40.267041       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I0410 13:38:40.278536       1 controller.go:611] quota admission added evaluator for: deployments.apps
I0410 13:38:40.300126       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I0410 13:38:40.308225       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0410 13:38:40.311212       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0410 13:38:43.081268       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
I0410 13:38:52.071796       1 controller.go:611] quota admission added evaluator for: endpoints
I0410 13:38:52.567835       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0410 13:41:09.946659       1 trace.go:205] Trace[880028028]: "Get" url:/api/v1/namespaces/default/pods/bb-helm-chart-nodeapp-f466cbff5-jdddc/log,user-agent:kubectl/v1.22.5 (darwin/arm64) kubernetes/5c99e2a,audit-id:c0f7e4c1-7109-4d6b-9bd7-ec42b2be2b5a,client:192.168.49.1,accept:application/json, */*,protocol:HTTP/2.0 (10-Apr-2022 13:40:52.876) (total time: 17070ms):
Trace[880028028]: ---"Transformed response object" 17068ms (13:41:09.946)
Trace[880028028]: [17.070158674s] [17.070158674s] END

* 
* ==> kube-controller-manager [babd5250bccc] <==
* I0409 08:42:25.114015       1 shared_informer.go:247] Caches are synced for garbage collector 
I0409 08:42:25.114173       1 garbagecollector.go:151] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
E0409 10:14:59.943446       1 resource_quota_controller.go:413] failed to discover resources: Unauthorized
W0409 10:15:00.357677       1 garbagecollector.go:705] failed to discover preferred resources: Unauthorized
W0409 11:27:33.803589       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "default/dd-helm-chart-nodeapp", retrying. Error: failed to create EndpointSlice for Service default/dd-helm-chart-nodeapp: Unauthorized
I0409 11:27:33.803810       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp" kind="Service" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpointSlices" message="Error updating Endpoint Slices for Service default/dd-helm-chart-nodeapp: failed to create EndpointSlice for Service default/dd-helm-chart-nodeapp: Unauthorized"
I0409 11:27:33.823116       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Warning" reason="ReplicaSetCreateError" message="Failed to create new replica set \"dd-helm-chart-nodeapp-5847d7ccf9\": Unauthorized"
I0409 11:27:33.827485       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dd-helm-chart-nodeapp-5847d7ccf9 to 1"
I0409 11:27:33.828692       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp-5847d7ccf9" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: Unauthorized"
E0409 11:27:33.840386       1 replica_set.go:536] sync "default/dd-helm-chart-nodeapp-5847d7ccf9" failed with Unauthorized
I0409 11:27:33.844572       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp-5847d7ccf9" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dd-helm-chart-nodeapp-5847d7ccf9-btlxk"
E0409 11:56:20.243655       1 resource_quota_controller.go:413] failed to discover resources: Unauthorized
W0409 11:56:20.700897       1 garbagecollector.go:705] failed to discover preferred resources: Unauthorized
I0409 13:52:58.036613       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dd-helm-chart-nodeapp-97d4d5bc4 to 1"
I0409 13:52:58.056711       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp-97d4d5bc4" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dd-helm-chart-nodeapp-97d4d5bc4-ln52b"
I0409 14:07:31.331893       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dd-helm-chart-nodeapp-85968645cd to 1"
I0409 14:07:31.339960       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp-85968645cd" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dd-helm-chart-nodeapp-85968645cd-zt2r7"
I0409 14:13:41.258928       1 event.go:291] "Event occurred" object="default/bb-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set bb-helm-chart-nodeapp-5c95f77bd6 to 1"
I0409 14:13:41.267105       1 event.go:291] "Event occurred" object="default/bb-helm-chart-nodeapp-5c95f77bd6" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: bb-helm-chart-nodeapp-5c95f77bd6-txrlz"
I0409 14:20:02.125535       1 event.go:291] "Event occurred" object="default/bb-helm-chart-nodeapp-5c95f77bd6" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: bb-helm-chart-nodeapp-5c95f77bd6-rz92j"
E0409 16:36:59.345662       1 resource_quota_controller.go:413] failed to discover resources: Unauthorized
W0409 16:36:59.345750       1 garbagecollector.go:705] failed to discover preferred resources: Unauthorized
E0409 19:28:02.169852       1 resource_quota_controller.go:413] failed to discover resources: Unauthorized
W0409 19:28:02.170327       1 garbagecollector.go:705] failed to discover preferred resources: Unauthorized
W0409 22:35:24.401356       1 garbagecollector.go:705] failed to discover preferred resources: Unauthorized
E0409 22:35:24.401367       1 resource_quota_controller.go:413] failed to discover resources: Unauthorized
E0410 01:01:44.797161       1 resource_quota_controller.go:413] failed to discover resources: Unauthorized
W0410 01:01:44.797484       1 garbagecollector.go:705] failed to discover preferred resources: Unauthorized
W0410 02:03:17.649967       1 garbagecollector.go:705] failed to discover preferred resources: Unauthorized
E0410 02:03:17.650068       1 resource_quota_controller.go:413] failed to discover resources: Unauthorized
E0410 03:31:53.265998       1 resource_quota_controller.go:413] failed to discover resources: Unauthorized
W0410 03:31:53.266021       1 garbagecollector.go:705] failed to discover preferred resources: Unauthorized
E0410 04:51:22.626687       1 resource_quota_controller.go:413] failed to discover resources: Unauthorized
W0410 04:51:22.626954       1 garbagecollector.go:705] failed to discover preferred resources: Unauthorized
I0410 06:08:28.248768       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dd-helm-chart-nodeapp-788f4c77bd to 1"
I0410 06:08:28.262944       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp-788f4c77bd" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dd-helm-chart-nodeapp-788f4c77bd-pwhzw"
I0410 06:10:10.779486       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dd-helm-chart-nodeapp-788f4c77bd to 1"
I0410 06:10:10.787884       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp-788f4c77bd" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dd-helm-chart-nodeapp-788f4c77bd-569lw"
I0410 06:12:42.256313       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dd-helm-chart-nodeapp-788f4c77bd to 1"
I0410 06:12:42.263839       1 event.go:291] "Event occurred" object="default/dd-helm-chart-nodeapp-788f4c77bd" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dd-helm-chart-nodeapp-788f4c77bd-9p2zk"
I0410 06:13:09.007235       1 event.go:291] "Event occurred" object="default/aa-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set aa-helm-chart-nodeapp-97d6fc644 to 1"
I0410 06:13:09.012564       1 event.go:291] "Event occurred" object="default/aa-helm-chart-nodeapp-97d6fc644" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: aa-helm-chart-nodeapp-97d6fc644-b86ws"
I0410 06:15:26.168207       1 event.go:291] "Event occurred" object="default/aa-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set aa-helm-chart-nodeapp-97d6fc644 to 1"
I0410 06:15:26.174426       1 event.go:291] "Event occurred" object="default/aa-helm-chart-nodeapp-97d6fc644" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: aa-helm-chart-nodeapp-97d6fc644-c8d2b"
E0410 06:16:26.237450       1 pv_controller.go:1451] error finding provisioning plugin for claim default/mongo-pvc: storageclass.storage.k8s.io "mylocalstorage" not found
I0410 06:16:26.237660       1 event.go:291] "Event occurred" object="default/mongo-pvc" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"mylocalstorage\" not found"
I0410 06:17:15.261793       1 event.go:291] "Event occurred" object="default/mongodb-deployment" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mongodb-deployment-7544874b58 to 1"
I0410 06:17:15.270304       1 event.go:291] "Event occurred" object="default/mongodb-deployment-7544874b58" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mongodb-deployment-7544874b58-vgbp5"
I0410 06:24:56.803899       1 event.go:291] "Event occurred" object="default/aa-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set aa-helm-chart-nodeapp-97d6fc644 to 1"
I0410 06:24:56.811796       1 event.go:291] "Event occurred" object="default/aa-helm-chart-nodeapp-97d6fc644" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: aa-helm-chart-nodeapp-97d6fc644-bqk8f"
I0410 06:29:41.401308       1 event.go:291] "Event occurred" object="default/aa-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set aa-helm-chart-nodeapp-8d9c79648 to 1"
I0410 06:29:41.414565       1 event.go:291] "Event occurred" object="default/aa-helm-chart-nodeapp-8d9c79648" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: aa-helm-chart-nodeapp-8d9c79648-86h5h"
I0410 06:29:51.529471       1 event.go:291] "Event occurred" object="default/aa-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set aa-helm-chart-nodeapp-97d6fc644 to 0"
I0410 06:29:51.538262       1 event.go:291] "Event occurred" object="default/aa-helm-chart-nodeapp-97d6fc644" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: aa-helm-chart-nodeapp-97d6fc644-bqk8f"
W0410 06:29:52.248774       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "default/aa-helm-chart-nodeapp", retrying. Error: EndpointSlice informer cache is out of date
I0410 06:45:40.326587       1 event.go:291] "Event occurred" object="default/aa-helm-chart-nodeapp-8d9c79648" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: aa-helm-chart-nodeapp-8d9c79648-2c84j"
W0410 06:45:40.983352       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "default/aa-helm-chart-nodeapp", retrying. Error: EndpointSlice informer cache is out of date
I0410 07:52:32.139309       1 event.go:291] "Event occurred" object="default/bb-helm-chart-nodeapp" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set bb-helm-chart-nodeapp-f466cbff5 to 1"
I0410 07:52:32.156320       1 event.go:291] "Event occurred" object="default/bb-helm-chart-nodeapp-f466cbff5" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: bb-helm-chart-nodeapp-f466cbff5-vjfj5"
I0410 07:57:50.615943       1 event.go:291] "Event occurred" object="default/bb-helm-chart-nodeapp-f466cbff5" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: bb-helm-chart-nodeapp-f466cbff5-5rfxw"

* 
* ==> kube-controller-manager [fbce405c0058] <==
* I0410 13:38:51.917675       1 dynamic_serving_content.go:129] "Starting controller" name="csr-controller::/var/lib/minikube/certs/ca.crt::/var/lib/minikube/certs/ca.key"
I0410 13:38:51.965968       1 controllermanager.go:577] Started "persistentvolume-binder"
I0410 13:38:51.966083       1 pv_controller_base.go:308] Starting persistent volume controller
I0410 13:38:51.968466       1 shared_informer.go:240] Waiting for caches to sync for persistent volume
I0410 13:38:51.975761       1 shared_informer.go:240] Waiting for caches to sync for resource quota
I0410 13:38:51.977267       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0410 13:38:51.978637       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
W0410 13:38:51.981380       1 actual_state_of_world.go:534] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I0410 13:38:51.982043       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0410 13:38:51.982359       1 job_controller.go:406] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0410 13:38:51.987808       1 shared_informer.go:240] Waiting for caches to sync for garbage collector
I0410 13:38:52.011951       1 shared_informer.go:247] Caches are synced for cronjob 
I0410 13:38:52.016297       1 shared_informer.go:247] Caches are synced for PV protection 
I0410 13:38:52.016316       1 shared_informer.go:247] Caches are synced for endpoint 
I0410 13:38:52.017029       1 shared_informer.go:247] Caches are synced for GC 
I0410 13:38:52.027111       1 shared_informer.go:247] Caches are synced for TTL after finished 
I0410 13:38:52.027166       1 shared_informer.go:247] Caches are synced for node 
I0410 13:38:52.027176       1 range_allocator.go:172] Starting range CIDR allocator
I0410 13:38:52.027185       1 shared_informer.go:240] Waiting for caches to sync for cidrallocator
I0410 13:38:52.027188       1 shared_informer.go:247] Caches are synced for cidrallocator 
I0410 13:38:52.029439       1 shared_informer.go:247] Caches are synced for job 
I0410 13:38:52.030509       1 shared_informer.go:247] Caches are synced for deployment 
I0410 13:38:52.031523       1 shared_informer.go:247] Caches are synced for bootstrap_signer 
I0410 13:38:52.037818       1 shared_informer.go:247] Caches are synced for attach detach 
I0410 13:38:52.038014       1 shared_informer.go:247] Caches are synced for PVC protection 
I0410 13:38:52.041375       1 shared_informer.go:247] Caches are synced for ReplicaSet 
I0410 13:38:52.042651       1 shared_informer.go:247] Caches are synced for stateful set 
I0410 13:38:52.043714       1 shared_informer.go:247] Caches are synced for TTL 
I0410 13:38:52.044821       1 shared_informer.go:247] Caches are synced for daemon sets 
I0410 13:38:52.048004       1 shared_informer.go:247] Caches are synced for HPA 
I0410 13:38:52.050175       1 shared_informer.go:247] Caches are synced for crt configmap 
I0410 13:38:52.052421       1 shared_informer.go:247] Caches are synced for expand 
I0410 13:38:52.066133       1 shared_informer.go:247] Caches are synced for ClusterRoleAggregator 
I0410 13:38:52.066402       1 shared_informer.go:247] Caches are synced for service account 
I0410 13:38:52.066487       1 shared_informer.go:247] Caches are synced for ephemeral 
I0410 13:38:52.070727       1 shared_informer.go:247] Caches are synced for namespace 
I0410 13:38:52.071652       1 shared_informer.go:247] Caches are synced for persistent volume 
I0410 13:38:52.116802       1 shared_informer.go:247] Caches are synced for taint 
I0410 13:38:52.116828       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I0410 13:38:52.116842       1 node_lifecycle_controller.go:1398] Initializing eviction metric for zone: 
W0410 13:38:52.116879       1 node_lifecycle_controller.go:1013] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0410 13:38:52.116915       1 node_lifecycle_controller.go:1214] Controller detected that zone  is now in state Normal.
I0410 13:38:52.116991       1 event.go:291] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0410 13:38:52.155808       1 shared_informer.go:247] Caches are synced for certificate-csrapproving 
I0410 13:38:52.166354       1 shared_informer.go:247] Caches are synced for disruption 
I0410 13:38:52.166396       1 disruption.go:371] Sending events to api server.
I0410 13:38:52.166649       1 shared_informer.go:247] Caches are synced for ReplicationController 
I0410 13:38:52.217248       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-serving 
I0410 13:38:52.217309       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-client 
I0410 13:38:52.217320       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kube-apiserver-client 
I0410 13:38:52.217733       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-legacy-unknown 
I0410 13:38:52.239976       1 shared_informer.go:247] Caches are synced for endpoint_slice 
I0410 13:38:52.261628       1 shared_informer.go:247] Caches are synced for endpoint_slice_mirroring 
I0410 13:38:52.274064       1 shared_informer.go:247] Caches are synced for resource quota 
I0410 13:38:52.278810       1 shared_informer.go:247] Caches are synced for resource quota 
W0410 13:38:52.570238       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "default/mongodb-service", retrying. Error: EndpointSlice informer cache is out of date
I0410 13:38:52.688684       1 shared_informer.go:247] Caches are synced for garbage collector 
I0410 13:38:52.755001       1 shared_informer.go:247] Caches are synced for garbage collector 
I0410 13:38:52.755078       1 garbagecollector.go:151] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0410 13:40:39.873577       1 event.go:291] "Event occurred" object="default/bb-helm-chart-nodeapp-f466cbff5" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: bb-helm-chart-nodeapp-f466cbff5-jdddc"

* 
* ==> kube-proxy [7a0111e619b0] <==
* I0409 08:42:16.411207       1 node.go:172] Successfully retrieved node IP: 192.168.49.2
I0409 08:42:16.411258       1 server_others.go:140] Detected node IP 192.168.49.2
W0409 08:42:16.411275       1 server_others.go:565] Unknown proxy mode "", assuming iptables proxy
I0409 08:42:16.444172       1 server_others.go:206] kube-proxy running in dual-stack mode, IPv4-primary
I0409 08:42:16.444244       1 server_others.go:212] Using iptables Proxier.
I0409 08:42:16.444274       1 server_others.go:219] creating dualStackProxier for iptables.
W0409 08:42:16.444293       1 server_others.go:495] detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6
I0409 08:42:16.445588       1 server.go:649] Version: v1.22.3
I0409 08:42:16.446730       1 config.go:224] Starting endpoint slice config controller
I0409 08:42:16.446768       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I0409 08:42:16.446832       1 config.go:315] Starting service config controller
I0409 08:42:16.446835       1 shared_informer.go:240] Waiting for caches to sync for service config
I0409 08:42:16.547423       1 shared_informer.go:247] Caches are synced for service config 
I0409 08:42:16.547584       1 shared_informer.go:247] Caches are synced for endpoint slice config 

* 
* ==> kube-proxy [eefb9eee263d] <==
* I0410 13:38:42.941341       1 node.go:172] Successfully retrieved node IP: 192.168.49.2
I0410 13:38:42.941380       1 server_others.go:140] Detected node IP 192.168.49.2
W0410 13:38:42.941398       1 server_others.go:565] Unknown proxy mode "", assuming iptables proxy
I0410 13:38:43.057459       1 server_others.go:206] kube-proxy running in dual-stack mode, IPv4-primary
I0410 13:38:43.057514       1 server_others.go:212] Using iptables Proxier.
I0410 13:38:43.057521       1 server_others.go:219] creating dualStackProxier for iptables.
W0410 13:38:43.057774       1 server_others.go:495] detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6
I0410 13:38:43.060474       1 server.go:649] Version: v1.22.3
I0410 13:38:43.062781       1 config.go:315] Starting service config controller
I0410 13:38:43.062791       1 shared_informer.go:240] Waiting for caches to sync for service config
I0410 13:38:43.062807       1 config.go:224] Starting endpoint slice config controller
I0410 13:38:43.062809       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I0410 13:38:43.163804       1 shared_informer.go:247] Caches are synced for endpoint slice config 
I0410 13:38:43.163839       1 shared_informer.go:247] Caches are synced for service config 

* 
* ==> kube-scheduler [596b935cd69f] <==
* I0409 08:42:08.429414       1 serving.go:347] Generated self-signed cert in-memory
W0409 08:42:11.970568       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0409 08:42:11.970674       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0409 08:42:11.970691       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W0409 08:42:11.970701       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0409 08:42:12.008464       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I0409 08:42:12.008536       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0409 08:42:12.010296       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0409 08:42:12.009001       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
E0409 08:42:12.016317       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"default\" not found" namespace="default"
E0409 08:42:12.016347       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"ingress-nginx\" not found" namespace="ingress-nginx"
E0409 08:42:12.016363       1 plugin.go:138] "getting namespace, assuming empty set of namespace labels" err="namespace \"kube-system\" not found" namespace="kube-system"
I0409 08:42:12.111059       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 

* 
* ==> kube-scheduler [7bad8e541125] <==
* I0410 13:38:35.930947       1 serving.go:347] Generated self-signed cert in-memory
W0410 13:38:38.957052       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0410 13:38:38.958118       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0410 13:38:38.958197       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W0410 13:38:38.958226       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0410 13:38:38.991158       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I0410 13:38:38.991615       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0410 13:38:38.991638       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0410 13:38:38.993208       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0410 13:38:39.093860       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 

* 
* ==> kubelet <==
* -- Logs begin at Sun 2022-04-10 13:38:26 UTC, end at Sun 2022-04-10 13:52:23 UTC. --
Apr 10 13:46:21 minikube kubelet[996]: E0410 13:46:21.493626     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:46:32 minikube kubelet[996]: I0410 13:46:32.493477     996 scope.go:110] "RemoveContainer" containerID="3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222"
Apr 10 13:46:32 minikube kubelet[996]: E0410 13:46:32.493711     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:46:44 minikube kubelet[996]: I0410 13:46:44.493382     996 scope.go:110] "RemoveContainer" containerID="3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222"
Apr 10 13:46:44 minikube kubelet[996]: E0410 13:46:44.493593     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:46:56 minikube kubelet[996]: I0410 13:46:56.492934     996 scope.go:110] "RemoveContainer" containerID="3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222"
Apr 10 13:46:56 minikube kubelet[996]: E0410 13:46:56.493271     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:47:07 minikube kubelet[996]: I0410 13:47:07.492712     996 scope.go:110] "RemoveContainer" containerID="3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222"
Apr 10 13:47:07 minikube kubelet[996]: E0410 13:47:07.492978     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:47:20 minikube kubelet[996]: I0410 13:47:20.493901     996 scope.go:110] "RemoveContainer" containerID="3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222"
Apr 10 13:47:20 minikube kubelet[996]: E0410 13:47:20.494346     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:47:31 minikube kubelet[996]: I0410 13:47:31.493350     996 scope.go:110] "RemoveContainer" containerID="3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222"
Apr 10 13:47:31 minikube kubelet[996]: E0410 13:47:31.493738     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:47:45 minikube kubelet[996]: I0410 13:47:45.492695     996 scope.go:110] "RemoveContainer" containerID="3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222"
Apr 10 13:47:45 minikube kubelet[996]: E0410 13:47:45.492929     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:48:00 minikube kubelet[996]: I0410 13:48:00.493476     996 scope.go:110] "RemoveContainer" containerID="3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222"
Apr 10 13:48:00 minikube kubelet[996]: E0410 13:48:00.493696     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:48:13 minikube kubelet[996]: I0410 13:48:13.493625     996 scope.go:110] "RemoveContainer" containerID="3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222"
Apr 10 13:48:13 minikube kubelet[996]: E0410 13:48:13.493898     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:48:25 minikube kubelet[996]: I0410 13:48:25.493310     996 scope.go:110] "RemoveContainer" containerID="3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222"
Apr 10 13:48:25 minikube kubelet[996]: E0410 13:48:25.493590     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:48:34 minikube kubelet[996]: W0410 13:48:34.489116     996 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 10 13:48:38 minikube kubelet[996]: I0410 13:48:38.493022     996 scope.go:110] "RemoveContainer" containerID="3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222"
Apr 10 13:48:38 minikube kubelet[996]: E0410 13:48:38.493418     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:48:50 minikube kubelet[996]: I0410 13:48:50.493609     996 scope.go:110] "RemoveContainer" containerID="3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222"
Apr 10 13:48:51 minikube kubelet[996]: I0410 13:48:51.429602     996 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/bb-helm-chart-nodeapp-f466cbff5-jdddc through plugin: invalid network status for"
Apr 10 13:49:19 minikube kubelet[996]: E0410 13:49:19.943473     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:49:20 minikube kubelet[996]: I0410 13:49:20.637437     996 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/bb-helm-chart-nodeapp-f466cbff5-jdddc through plugin: invalid network status for"
Apr 10 13:49:20 minikube kubelet[996]: I0410 13:49:20.641323     996 scope.go:110] "RemoveContainer" containerID="3dfe819fbcb7e1d551e3c309cc60aa64c95cbc57861bc3471809fe83cf9bb222"
Apr 10 13:49:20 minikube kubelet[996]: I0410 13:49:20.641522     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:49:20 minikube kubelet[996]: E0410 13:49:20.641729     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:49:21 minikube kubelet[996]: I0410 13:49:21.649426     996 docker_sandbox.go:401] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for default/bb-helm-chart-nodeapp-f466cbff5-jdddc through plugin: invalid network status for"
Apr 10 13:49:31 minikube kubelet[996]: I0410 13:49:31.493166     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:49:31 minikube kubelet[996]: E0410 13:49:31.493415     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:49:43 minikube kubelet[996]: I0410 13:49:43.493079     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:49:43 minikube kubelet[996]: E0410 13:49:43.493433     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:49:57 minikube kubelet[996]: I0410 13:49:57.493452     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:49:57 minikube kubelet[996]: E0410 13:49:57.493707     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:50:09 minikube kubelet[996]: I0410 13:50:09.492777     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:50:09 minikube kubelet[996]: E0410 13:50:09.492994     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:50:22 minikube kubelet[996]: I0410 13:50:22.493264     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:50:22 minikube kubelet[996]: E0410 13:50:22.493498     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:50:34 minikube kubelet[996]: I0410 13:50:34.493291     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:50:34 minikube kubelet[996]: E0410 13:50:34.493507     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:50:46 minikube kubelet[996]: I0410 13:50:46.493342     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:50:46 minikube kubelet[996]: E0410 13:50:46.493563     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:51:00 minikube kubelet[996]: I0410 13:51:00.492813     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:51:00 minikube kubelet[996]: E0410 13:51:00.493035     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:51:11 minikube kubelet[996]: I0410 13:51:11.493408     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:51:11 minikube kubelet[996]: E0410 13:51:11.493632     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:51:22 minikube kubelet[996]: I0410 13:51:22.492797     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:51:22 minikube kubelet[996]: E0410 13:51:22.493013     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:51:37 minikube kubelet[996]: I0410 13:51:37.492700     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:51:37 minikube kubelet[996]: E0410 13:51:37.492946     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:51:50 minikube kubelet[996]: I0410 13:51:50.493078     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:51:50 minikube kubelet[996]: E0410 13:51:50.493372     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:52:03 minikube kubelet[996]: I0410 13:52:03.492432     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:52:03 minikube kubelet[996]: E0410 13:52:03.492680     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc
Apr 10 13:52:15 minikube kubelet[996]: I0410 13:52:15.493515     996 scope.go:110] "RemoveContainer" containerID="2f4410795b334d6e7da088e7bb4a54c0dff14c6135f3e82efb3d3a518d14cdc3"
Apr 10 13:52:15 minikube kubelet[996]: E0410 13:52:15.493746     996 pod_workers.go:836] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"helm-chart-nodeapp\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=helm-chart-nodeapp pod=bb-helm-chart-nodeapp-f466cbff5-jdddc_default(103fc9b7-30d1-40fa-9a6e-6033cdba80cc)\"" pod="default/bb-helm-chart-nodeapp-f466cbff5-jdddc" podUID=103fc9b7-30d1-40fa-9a6e-6033cdba80cc

* 
* ==> storage-provisioner [306e5b763257] <==
* I0410 13:38:41.628160       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0410 13:39:11.633858       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> storage-provisioner [a22c0402f828] <==
* I0410 13:39:23.588634       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0410 13:39:23.601689       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0410 13:39:23.602384       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0410 13:39:41.021152       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0410 13:39:41.021256       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_16c06a9c-044a-456b-8b3b-96bd2efe4644!
I0410 13:39:41.021485       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"6afb1029-b0a2-4334-9730-3123967755d0", APIVersion:"v1", ResourceVersion:"125117", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_16c06a9c-044a-456b-8b3b-96bd2efe4644 became leader
I0410 13:39:41.122010       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_16c06a9c-044a-456b-8b3b-96bd2efe4644!
I0410 13:39:41.122271       1 controller.go:1472] delete "pvc-9108cc52-c5d9-42e6-9f83-8f455975d32f": started
I0410 13:39:41.122284       1 storage_provisioner.go:98] Deleting volume &PersistentVolume{ObjectMeta:{pvc-9108cc52-c5d9-42e6-9f83-8f455975d32f    34efebbf-7070-464b-829a-1338d36cd570 118014 0 2022-04-09 08:13:17 +0000 UTC <nil> <nil> map[] map[hostPathProvisionerIdentity:74aad659-a39b-4072-a5bc-dda3ab0db451 pv.kubernetes.io/provisioned-by:k8s.io/minikube-hostpath] [] [kubernetes.io/pv-protection]  [{kube-controller-manager Update v1 2022-04-09 08:13:17 +0000 UTC FieldsV1 {"f:status":{"f:phase":{}}}} {storage-provisioner Update v1 2022-04-09 08:13:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:hostPathProvisionerIdentity":{},"f:pv.kubernetes.io/provisioned-by":{}}},"f:spec":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:claimRef":{},"f:hostPath":{".":{},"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeSpec{Capacity:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},PersistentVolumeSource:PersistentVolumeSource{GCEPersistentDisk:nil,AWSElasticBlockStore:nil,HostPath:&HostPathVolumeSource{Path:/tmp/hostpath-provisioner/default/ee-mongodb,Type:*,},Glusterfs:nil,NFS:nil,RBD:nil,ISCSI:nil,Cinder:nil,CephFS:nil,FC:nil,Flocker:nil,FlexVolume:nil,AzureFile:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Local:nil,StorageOS:nil,CSI:nil,},AccessModes:[ReadWriteOnce],ClaimRef:&ObjectReference{Kind:PersistentVolumeClaim,Namespace:default,Name:ee-mongodb,UID:9108cc52-c5d9-42e6-9f83-8f455975d32f,APIVersion:v1,ResourceVersion:101579,FieldPath:,},PersistentVolumeReclaimPolicy:Delete,StorageClassName:standard,MountOptions:[],VolumeMode:*Filesystem,NodeAffinity:nil,},Status:PersistentVolumeStatus{Phase:Released,Message:,Reason:,},}
I0410 13:39:41.125837       1 controller.go:1478] delete "pvc-9108cc52-c5d9-42e6-9f83-8f455975d32f": volume deletion ignored: ignored because identity annotation on PV does not match ours

